import argparse
import copy
import glob
import numpy as np
import os
import pandas as pd
import pkg_resources
import pickle as pkl
import re
import subprocess
import sys
import time

from Bio import Entrez
from collections import OrderedDict, Counter
from functools import partial
from multiprocessing import Pool, cpu_count
from pkg_resources import resource_filename

if 'postnovo' in sys.modules:
    import postnovo.config as config
    import postnovo.utils as utils
else:
    import config
    import utils

local_eggnog = False
fasta_input_fp = ''
out_dir = ''

min_seq_len_for_diamond = 31

Entrez.email = 'samuelmiller@uchicago.edu'
cores = 1

# Output field names specified in BLAST+
blast_out_hdr = [
    'qseqid',
    'accession',
    'evalue',
    'bitscore', 
    'stitle'
    ]
# Output field names specified in DIAMOND
diamond_out_hdr = [
    'qseqid',
    'sseqid',
    'evalue',
    'bitscore',
    'staxids', 
    'stitle'
    ]
# Column names of merged DIAMOND and BLAST output
align_out_hdr = [
    'seq_number',
    'hit_number',
    'evalue',
    'bitscore',
    'taxid',
    'stitle'
    ]

max_float = np.finfo('f').max
max_int = np.finfo('d').max

superkingdoms = [
    'Archaea', 
    'Bacteria',
    'Eukaryota',
    'Viruses'
    ]
search_ranks = [
    'species', 
    'genus', 
    'family', 
    'order', 
    'class', 
    'phylum', 
    'superkingdom'
    ]
taxon_assignment_threshold = 0.9

emapper_abbr_dict = {
    'archaea': 'arch',
    'bacteria': 'bact', 
    'eukaryota': 'euk', 
    'viruses': 'viruses'
    }

eggnog_output_headers = [
    'query', 
    'seed ortholog', 
    'evalue', 
    'score', 
    'predicted name',
    'go terms', 
    'kegg pathways', 
    'tax scope', 
    'eggnog ogs', 
    'best og',
    'cog cat', 
    'eggnog hmm desc'
    ]

parsed_conserv_func_df_headers_list = [
    'seed ortholog', 
    'predicted name', 
    'go terms', 
    'kegg pathways',
    'tax scope', 
    'eggnog ogs', 
    'best og', 
    'cog cat', 
    'eggnog hmm desc'
    ]

# The e-value cutoff for "confident," "non-random" alignments
confident_evalue = 0.1

def main(test_argv=None):
    global local_eggnog, out_dir

    test_argv = []
    if 'test_argv' in locals():
        args = parse_args(test_argv)
    else:
        args = parse_args()

    if args.command == 'make_emapper_input':
        if args.local_eggnog:
            local_eggnog = True
        make_query_files(args)
    elif args.command == 'analyze_eggnog_output':
        out_dir = args.iodir

        # Load files generated by make_emapper_input
        sampled_df = pd.read_csv(
            os.path.join(out_dir, 'sampled_df.csv')
            )
        low_prob_profile_df = pd.read_csv(
            os.path.join(out_dir, 'low_prob_profile_df.csv')
            )
        high_prob_taxa_assign_df = pd.read_csv(
            os.path.join(out_dir, 'high_prob_taxa_assign_df.csv')
            )
        with open(os.path.join(out_dir, 'eggnog_fasta_path_list.pkl'), 'rb') as f:
            eggnog_fasta_path_list = pkl.load(f)

        analyze_eggnog_output(
            args, sampled_df, low_prob_profile_df, high_prob_taxa_assign_df, eggnog_fasta_path_list
            )

def make_query_files(args):
    
    global fasta_input_fp, out_dir
    fasta_list = open(args.postnovo_seqs, 'r').readlines()
    fasta_input_fp = args.postnovo_seqs

    out_dir = os.path.dirname(fasta_input_fp)

    # If BLAST+ and DIAMOND have already been run,
    if args.redo_without_search:

        out_dir_files = [f for f in os.listdir(out_dir) if os.path.isfile(os.path.join(out_dir, f))]
        diamond_out_fp = ''
        split_blast_out_fp_list = []
        fasta_basename = os.path.splitext(os.path.basename(fasta_input_fp))[0]
        for f in out_dir_files:
            if fasta_basename + '.diamond.out' in f:
                if diamond_out_fp != '':
                    raise Exception('DIAMOND output of the name ' + os.path.splitext(fasta_input_fp)[0] + '.diamond.out already exists')
            elif fasta_basename in f and 'blastp-short.out' in f and os.path.splitext(f)[1] == '.out':
                split_blast_out_fp_list.append(os.path.join(out_dir, f))

    else:

        # Check that the input fasta file looks like a fasta file
        parent_fasta_size = len(fasta_list) / 2
        if parent_fasta_size % int(parent_fasta_size) > 0:
            raise ValueError('The fasta input must have an even number of lines.')

        # Separate fasta entries into longer seqs for DIAMOND and shorter seqs for BLAST
        diamond_fasta_list = []
        blast_fasta_list = []
        entry = []
        for line in fasta_list:
            entry.append(line)
            if line[0] != '>':
                if len(line.rstrip()) >= min_seq_len_for_diamond:
                    diamond_fasta_list += entry
                else:
                    blast_fasta_list += entry
                entry = []

        # Write fasta file for DIAMOND
        diamond_fasta_fp = os.path.splitext(fasta_input_fp)[0] + '.diamond.faa'
        with open(diamond_fasta_fp, 'w') as handle:
            for line in diamond_fasta_list:
                handle.write(line)

        # To run BLAST+ on multiple CPUs, split the fasta input into multiple files
        split_blast_fasta_fp_list = split_blast_fasta(blast_fasta_list, args.max_seqs_per_blast_instance)

        #run_diamond(diamond_fasta_fp, args.diamond, args.diamond_db, args.taxonmap, args.cores)
        split_blast_out_fp_list = run_blast(args.blast_dbs, split_blast_fasta_fp_list, args.blastp)        

        sys.exit()

    # Make a table of sequence information to add to the alignment output
    seq_info_table = make_info_table(fasta_input_fp, args.postnovo_seqs_info)

    # Load the BLAST output from alignments against each db
    db_names = [os.path.basename(blast_db_fp) for blast_db_fp in args.blast_dbs]
    blast_out_dict = OrderedDict().fromkeys(db_names)
    for db_name in db_names:
        blast_out_table = pd.DataFrame(columns=blast_out_hdr)
        # Piece the BLAST table together from its parts
        for partial_blast_out_fp in split_blast_out_fp_list:
            partial_blast_out_table = pd.read_csv(
                partial_blast_out_fp, sep='\t', header=None, names=blast_out_hdr
                )
            blast_out_table = pd.concat(
                [blast_out_table, partial_blast_out_table], ignore_index=True
                )

        # Reorganize the qseqid column into the seq_number column
        blast_out_table['seq_number'] = blast_out_table['qseqid'].apply(
            lambda x: int(x.replace('(seq_number)', ''))
            )
        blast_out_table.drop('qseqid', inplace=True)
        # Add a hit_number column to keep track of the alignment rank
        blast_out_table['hit_number'] = make_hit_number_list(blast_out_table['seq_number'].tolist())

        # For debugging purposes, write the current db BLAST table to file
        if args.intermediate_files:
            blast_out_table.to_csv(
                os.path.join(out_dir, db_name + '_blast_out.csv'), index=False
                )

        # Add seq info to the blast output table
        blast_out_table = blast_out_table.merge(seq_info_table, on=['seq_number'])

        blast_out_dict[db_name] = blast_out_table

    # Compare results from multiple databases
    if len(blast_out_dict) == 1:
        blast_out_table = list(blast_out_dict.values())[0]
    else:
        blast_out_table = refine_blast_table(blast_out_dict)

    # Unzipped taxonmap file should be provided in the same dir as zipped file
    taxonmap_fp = os.path.splitext(args.taxonmap)[0]
    # List the accessions in the (refined) BLAST results
    out_accessions = list(set(blast_out_table['accession'].tolist()))
    taxid_dict = OrderedDict().fromkeys(out_accessions)
    with open(args.taxonmap) as handle:
        # Search the file for accessions until all have been found
        for line in handle:
            # The first field of accession2taxid is the accession
            accession = line.split('\t')[0]
            if accession in out_accessions:
                # The third field is the taxid
                taxid_dict[accession] = line.split('\t')[2]
                out_accessions.remove(accession)
                if not out_accessions:
                    break
    # Throw an exception if accessions remain unassigned
    if out_accessions:
        raise Exception(
            'The following accessions were not found in ' + 
            os.path.basename(taxonmap_fp) + ': ' +
            ', '.join(out_accessions)
            )
    taxid_list = []
    for accession in blast_out_table['accession'].tolist():
        taxid_list.append(taxid_dict[accession])
    blast_out_table['taxid'] = taxid_list
    # Rearrange the cols in a predictable order
    blast_out_table = blast_out_table[align_out_hdr]

    # Load the DIAMOND output
    diamond_out_table = pd.read_csv(
        os.path.splitext(fasta_input_fp)[0] + '.diamond.out', sep='\t', header=None, names=align_out_hdr
        )
    # Remove seqs without an alignment
    diamond_out_table = diamond_out_table[diamond_out_table['accession'] != '*']
    diamond_out_table['seq_number'] = diamond_out_table['qseqid'].apply(
        lambda x: x.replace('(seq_number)', '')
        )
    # Remove the version number from the accession to make it consistent with BLAST
    diamond_out_table['accession'] = diamond_out_table['sseqid'].apply(lambda x: x.split('.')[0])
    diamond_out_table.drop('sseqid', inplace=True)
    # Add a hit number column
    diamond_out_table['hit_number'] = make_hit_number_list(diamond_out_table['seq_number'].tolist())
    # Rename 'staxids' to 'taxid' to be consistent with BLAST
    diamond_out_table['taxid'] = diamond_out_table['staxids']
    diamond_out_table.drop('staxids', inplace=True)
    # Remove the accession.version substring from stitle to make it consistent with BLAST
    diamond_out_table['stitle'] = diamond_out_table['stitle'].apply(lambda x: x[x.index(' ') + 1:])
    # Rearrange the cols in a predictable order
    diamond_out_table = diamond_out_table[align_out_hdr]

    # Combine DIAMOND and BLAST output
    align_out_table = pd.concat([diamond_out_table, blast_out_table], ignore_index=True)
    align_out_table.sort_values(['seq_number', 'hit'], inplace=True)

    # REMOVE
    # For debugging, report any seqs that return multiple taxids:
    # these should be separated by a semicolon (according to DIAMOND manual),
    # and the column datatype is therefore an object rather than numeric
    if not pd.api.types.is_numeric_dtype(align_out_table['taxid']):
        print(align_out_table[align_out_table['taxid'].apply(lambda x: ';' in x)], flush=True)

    # For debugging purposes, write the alignment output table to file
    if args.intermediate_files:
        align_out_table.to_csv(
            os.path.join(out_dir, 'align_out.csv'), index=False
            )

    # Test the data types of the columns to avoid unexpected surprises
    assert pd.api.types.is_numeric_dtype(align_out_table['seq_number'])
    assert pd.api.types.is_numeric_dtype(align_out_table['hit_number'])
    assert pd.api.types.is_object_dtype(align_out_table['accession'])
    assert pd.api.types.is_numeric_dtype(align_out_table['evalue'])
    assert pd.api.types.is_numeric_dtype(align_out_table['bitscore'])
    assert pd.api.types.is_numeric_dtype(align_out_table['taxid'])
    assert pd.api.types.is_object_dtype(align_out_table['stitle'])

    # Filter the alignment output table by evalue,
    # sorting into high- and low-probability sub-tables
    high_prob_out_table, lower_prob_out_table, filtered_out_table = filter_align_out_table(align_out_table)

    if args.intermediate_files:
        high_prob_out_table.to_csv(
            os.path.join(out_dir, 'high_prob_out.csv'), index=False
            )
        lower_prob_out_table.to_csv(
            os.path.join(out_dir, 'lower_prob_out.csv'), index=False
            )
        filtered_out_table.to_csv(
            os.path.join(out_dir, 'filtered_out.csv'), index=False
            )

    high_prob_out_table, lower_prob_out_table, filtered_out_table = get_linnean_hierarchy(
        high_prob_out_table, lower_prob_out_table, filtered_out_table
        )

    if args.intermediate_files:
        high_prob_out_table.to_csv(
            os.path.join(out_dir, 'high_prob_out_with_lineages.csv'), index=False
            )
        lower_prob_out_table.to_csv(
            os.path.join(out_dir, 'lower_prob_out_with_lineages.csv'), index=False
            )
        filtered_out_table.to_csv(
            os.path.join(out_dir, 'filtered_out_table_with_lineages.csv'), index=False
            )

    #high_prob_taxa_assign_df, high_prob_taxa_count_df = find_parsimonious_taxonomy(high_prob_df)
    #high_prob_taxa_assign_df.to_csv(
    #    os.path.join(out_dir, 'high_prob_taxa_assign_df.csv'), index=False
    #    )
    #if args.intermediate_files:
    #    high_prob_taxa_count_df.to_csv(
    #        os.path.join(out_dir, 'high_prob_taxa_count_df.csv'), index=False
    #        )
    #profile_df, low_prob_profile_df = screen_taxonomic_profile(
    #    high_prob_taxa_assign_df, high_prob_taxa_count_df, low_prob_df, high_prob_df
    #    )
    #if args.intermediate_files:
    #    profile_df.to_csv(
    #        os.path.join(out_dir, 'profile_df.csv'), index=False
    #        )
    #low_prob_profile_df.to_csv(
    #    os.path.join(out_dir, 'low_prob_profile_df.csv'), index=False
    #    )

    ## Test the functional uniformity of each set of hits to determine hit accuracy
    ## Draw up to 10 hits from the high-scoring hits of each query group
    ## Evenly sample groups larger than 10, starting with hit 0
    #scan_list_groups = profile_df.groupby(id_type)
    #sampled_df = scan_list_groups.apply(sample_hits)
    #sampled_df.drop('is_in_profile', axis=1, inplace=True)
    #sampled_df.to_csv(
    #    os.path.join(out_dir, 'sampled_df.csv'), index=False
    #    )

    ## Make fasta files for eggnog-mapper
    ## Header: >(scan_list)scan lists(hit)hit number
    ## Seq: full subject seq for each hit
    ## Sort into fasta files based on superkingdom (4 types of files total)
    #generic_emapper_fasta_fp = os.path.join(
    #    out_dir, os.path.splitext(os.path.basename(fasta_input_fp))[0] + '_eggnog_mapper.faa'
    #    )
    #eggnog_fasta_path_list = make_full_hit_seq_fasta(sampled_df, generic_emapper_fasta_fp)
    #with open(
    #    os.path.join(out_dir, 'eggnog_fasta_path_list.pkl'), 'wb'
    #    ) as f:
    #    pkl.dump(eggnog_fasta_path_list, f, 2)

    #return sampled_df, low_prob_profile_df, high_prob_taxa_assign_df, eggnog_fasta_path_list

def make_hit_number_list(id_list):

    hit_number = 0
    hit_number_list = [0]
    last_id = 0
    for id in id_list[1:]:
        if id != last_id:
            hit_number = 0
            hit_number_list.append(hit_number)
            last_id = id
        else:
            hit_number += 1
            hit_number_list.append(hit_number)
    blast_out_table['hit_number'] = hit_number_list

    return hit_number_list

def make_info_table(fasta_input_fp, info_fp):

    # Retrieve the query sequences from the postnovo fasta file
    with open(fasta_input_fp) as handle:
        fasta_input_list = handle.readlines()
    query_seqs = [seq.rstrip() for seq in fasta_input_list[1::2]]

    # Retrieve scan list, score, and best prediction and subseq origin lists
    # for each seq from postnovo_seqs_info.tsv
    info_table = pd.read_csv(info_fp, sep='\t', header=0)
    info_table['query_seq'] = query_seqs

    return info_table

def refine_blast_table(blast_out_dict):

    # Make a list of all the seq numbers in the BLAST output
    # (This ignores seqs that are not reported at all
    # when the alignment has an e-value greater than the threshold.)
    # Create a groupby (seq number) object for each blast table
    seq_numbers = []
    groupby_dict = OrderedDict().fromkeys(blast_out_dict)
    for db_name, blast_out_table in blast_out_dict.items():
        seq_numbers += blast_out_table['seq_number'].tolist()
        groupby_dict[db_name] = blast_out_table.groupby('seq_number')
    seq_number_set = set(seq_numbers)
    
    # Loop through each seq to consider alignments against the different db's
    # First, screen by a "non-random alignment" e-value cutoff
    # If a seq has one or more alignments below this cutoff,
    # take the results from the database with the lowest e-value
    # Second, if no seq has an alignment below this cutoff,
    # take the results from the first (most general) database
    refined_blast_results = []
    for seq_number in seq_number_set:
        lowest_evalue = max_float
        optimum_db_name = None
        for db_name, groupby_table in groupby_dict.items():
            # The table may not have an alignment for the seq
            try:
                evalue = groupby_table.get_group(seq_number)['evalue'].min()
                if evalue < lowest_evalue:
                    lowest_evalue = evalue
                    optimum_db = db_name
            except KeyError:
                pass
            # Store the db results in a list to be concatenated into a full table
            if lowest_evalue < confident_evalue:
                refined_blast_results.append(
                    groupby_dict[optimum_db].get_group(seq_number)
                    )
            else:
                # Find the most general db (db's should be in order of generality)
                # containing an alignment for any seq that does not meet the e-value cutoff
                for _, groupby_table1 in groupby_dict.items():
                    try:
                        refined_blast_results.append(
                            groupby_table1.get_group(seq_number)
                            )
                        break
                    except KeyError:
                        pass
    refined_blast_table = pd.concat(refined_blast_results, ignore_index=True)

    return refined_blast_table

def filter_align_out_table(align_out_table):

    seq_numbers = sorted(list(set(align_out_table['seq_number'].tolist())))
    high_prob_subtables = []
    lower_prob_subtables = []
    all_filtered_subtables = []
    groupby_table = align_out_table.groupby('seq_number')
    for seq_number in seq_numbers:
        subtable = groupby_table.get_group(seq_number)
        # For query seqs with an alignment meeting the e-value cutoff,
        # retain all alignments sharing the best bit score
        if subtable['evalue'].min() <= confident_evalue:
            high_prob_subtable = subtable[subtable['bitscore'] == subtable['bitscore'].max()]
            high_prob_subtables.append(high_prob_subtable)
            all_filtered_subtables.append(high_prob_subtable)
        # Otherwise retain all reported alignments (up to 500 in the case of BLAST+)
        else:
            lower_prob_subtables.append(subtable)
            all_filtered_subtables.append(subtable)

    high_prob_out_table = pd.concat(high_prob_subtables, ignore_index=True)
    lower_prob_out_table = pd.concat(lower_prob_subtables, ignore_index=True)
    filtered_out_table = pd.concat(all_filtered_subtables, ignore_index=True)

    return high_prob_out_table, lower_prob_out_table, filtered_out_table

def analyze_eggnog_output(args, sampled_df, low_prob_profile_df, high_prob_taxa_assign_df, eggnog_fasta_path_list):

    # Load as dataframe
    # Assign predefined column names
    # Concat annotation dfs
    eggnog_out_fp_list = [
        os.path.join(args.iodir, fasta + '.emapper.annotations') for fasta in eggnog_fasta_path_list
        ]

    eggnog_df = parse_eggnog_mapper_output(eggnog_out_fp_list)
    conserv_func_df = find_coherent_hits(eggnog_df, sampled_df)
    reported_df = condense_hits(conserv_func_df, low_prob_profile_df, high_prob_taxa_assign_df)
    reported_df.to_csv(
        os.path.join(args.iodir, 'reported_df.tsv'),
        sep='\t',
        index=False
        )

def parse_args(test_argv = None):

    parser = argparse.ArgumentParser(
        description = 'BLAST+ and taxonomic identification of peptide sequences'
        )
    subparsers = parser.add_subparsers(dest='command')

    parser_make_emapper_input = subparsers.add_parser(
        'make_emapper_input',
        help=('make fasta files for eggNOG-mapper input: '
              'step 1 of remote_eggnog procedure')
        )

    make_emapper_input_seq_source_group = parser_make_emapper_input.add_mutually_exclusive_group()
    make_emapper_input_seq_source_group.add_argument(
        '--postnovo_seqs',
        help='path to postnovo_seqs.faa, the fasta file of query sequences generated by postnovo'
        )
    parser_make_emapper_input.add_argument(
        '--postnovo_seqs_info',
        help='path to postnovo_seqs_info.tsv, produced at the same time as postnovo_seqs.faa'
        )

    parser_make_emapper_input.add_argument(
        '--diamond',
        help=(
            'path to DIAMOND executable: '
            'DIAMOND is used for fast alignment of query sequences longer than 30 AAs'
            )
        )
    parser_make_emapper_input.add_argument(
        '--diamond_db',
        help=(
            'DIAMOND database path as it would be specified in a DIAMOND search, '
            'e.g., /home/samuelmiller/diamond_db/refseq/refseq.dmnd'
            )
        )
    parser_make_emapper_input.add_argument(
        '--blastp',
        help=(
            'path to blastp executable: '
            'BLAST+ blastp-short is used for alignment of query sequences shorter than 30 AAs'
            )
        )
    parser_make_emapper_input.add_argument(
        '--blast_dbs',
        nargs='+',
        help=(
            'BLAST database paths as they would be specified in a BLAST+ search, '
            'e.g., /home/samuelmiller/blast_db/refseq_protein/refseq_protein '
            'If multiple databases are used, '
            'place in order of most general to specific, '
            'e.g., full RefSeq precedes bacterial RefSeq. '
            'Databases should be constructed from fasta files with makeblastdb '
            'after converting all occurrences of Ile to Leu. '
            'The database (not fasta) files should be in their own directory.'
            )
        )
    parser_make_emapper_input.add_argument(
        '--max_seqs_per_blast_instance',
        type=int,
        default=1000,
        help='maximum number of query seqs per BLAST+ instance'
        )
    parser_make_emapper_input.add_argument(
        '--taxonmap',
        help=(
            'Filepath to prot.accession2taxid.gz (from NCBI) '
            'for retrieving taxonomy from accession IDs: '
            'both zipped and unzipped files should be present in the directory '
            )
        )
    parser_make_emapper_input.add_argument(
        '--local_eggnog',
        default=False,
        action='store_true',
        help='flag to create files for local eggNOG'
        )
    parser_make_emapper_input.add_argument(
        '--cores',
        type=int,
        default=1,
        help='number of cores to use'
        )
    parser_make_emapper_input.add_argument(
        '--intermediate_files',
        default=False,
        action='store_true',
        help='flag to write intermediate files for debugging'
        )
    parser_make_emapper_input.add_argument(
        '--redo_without_search',
        default=False,
        action='store_true',
        help='flag to make fasta files after DIAMOND and BLAST output have been generated'
        )

    parser_analyze_emapper_output = subparsers.add_parser(
        'analyze_emapper_output',
        help='analyze eggNOG-mapper + HMMR annotation output'
        )
    parser_analyze_emapper_output.add_argument(
        '--iodir',
        help=('directory containing sampled_df.csv, '
              'low_prob_profile_df.csv, '
              'high_prob_taxa_assign_df.csv, '
              'and eggnog_fasta_path_list.pkl (all produced by make_emapper_input step), '
              'and eggNOG-mapper annotation files')
        )

    if test_argv:
        args = parser.parse_args(test_argv)
    else:
        args = parser.parse_args()
        check_args(parser, args)

    set_global_vars(args)

    return args

def set_global_vars(args):

    global cores

    if args.command == 'make_emapper_input':
        cores = args.cores

def check_args(parser, args):

    if args.command == 'make_emapper_input':

        if args.postnovo_seqs == None:
            parser.error('query fasta input must be provided')
        if args.postnovo_seqs != None:
            if not os.path.exists(args.postnovo_seqs):
                parser.error(args.postnovo_seqs + ' does not exist')

        if args.redo_without_search == False:
            if args.diamond == None:
                parser.error('diamond filepath needed')
            if not os.path.exists(args.diamond):
                parser.error(args.diamond + ' does not exist')
            if args.diamond_db == None:
                parser.error('DIAMOND database filepath must be provided')
            if not os.path.exists(args.diamond_db):
                parser.error(args.diamond_db + ' does not exist')
            if args.taxonmap == None:
                parser.error('taxonmap filepath must be provided')
            if not os.path.exists(args.taxonmap):
                parser.error(args.taxonmap + ' does not exist')

            if args.blastp == None:
                parser.error('blastp filepath needed')
            if not os.path.exists(args.blastp):
                parser.error(args.blastp + ' does not exist')
            if args.blast_dbs == None:
                parser.error('BLAST+ database filepath(s) must be provided')
            for db_fp in args.blast_dbs:
                db_dir = os.path.dirname(db_fp)
                db_name = os.path.basename(db_fp)
                if not os.path.exists(db_dir):
                    parser.error(db_dir + ' does not exist')
                if not os.path.exists(os.path.join(db_dir, db_name + '.pal')):
                    parser.error(db_name + ' does not exist in ' + db_dir)

        if args.max_seqs_per_blast_instance < 1:
            parser.error(str(args.max_seqs_per_blast_instance) + ' must be a positive number')

        if args.cores < 1 or args.cores > cpu_count():
            parser.error(str(cpu_count()) + ' cores are available')

    # analyze_eggnog_output subcommand
    elif args.command == 'analyze_emapper_output':
        if not os.path.exists(args.iodir):
            parser.error(args.iodir + ' does not exist')
        if (
            not os.path.exists(os.path.join(args.iodir, 'sampled_df.csv')) or 
            not os.path.exists(os.path.join(args.iodir, 'low_prob_profile_df.csv')) or
            not os.path.exists(os.path.join(args.iodir, 'high_prob_taxa_assign_df.csv')) or 
            not os.path.exists(os.path.join(args.iodir, 'eggnog_fasta_path_list.pkl'))
            ):
            parser.error(
                'the following files must be present in ' + 
                args.iodir + 
                ': sampled_df.csv, '
                'low_prob_profile_df.csv, '
                'high_prob_taxa_assign_df.csv, '
                'eggnog_fasta_path_list.pkl'
                )

def split_blast_fasta(fasta_list, max_seqs_per_process):
    '''
    To run multiple BLAST+ processes at once,
    split the fasta file into a number equal to the cores available.
    Returns a list of the new fasta filepaths.
    '''

    split_fasta_fp_list = []
    fasta_basename = os.path.splitext(
        os.path.basename(fasta_input_fp)
        )[0]
    parent_fasta_size = len(fasta_list) / 2
    if parent_fasta_size % int(parent_fasta_size) > 0:
        raise ValueError('The fasta input must have an even number of lines.')
    child_fasta_size = int(parent_fasta_size / cores)
    remainder = parent_fasta_size % cores

    if child_fasta_size + remainder < max_seqs_per_process:
        for core in range(cores):
            child_fasta_list = fasta_list[
                core * child_fasta_size * 2: (core + 1) * child_fasta_size * 2
                ]
            child_fasta_fp = os.path.join(
                out_dir, fasta_basename + '_' + str(core + 1) + '.blastp-short.faa'
                )
            with open(child_fasta_fp, 'w') as child_fasta_file:
                for line in child_fasta_list:
                    child_fasta_file.write(line)
            split_fasta_fp_list.append(child_fasta_fp)
        with open(child_fasta_fp, 'a') as child_fasta_file:
            child_fasta_list = fasta_list[cores * child_fasta_size * 2:]
            for line in child_fasta_list:
                child_fasta_file.write(line)
    else:
        fasta_line = 0
        child_fasta_count = 1
        while fasta_line < len(fasta_list):
            child_fasta_list = fasta_list[
                fasta_line: fasta_line + max_seqs_per_process * 2
                ]
            child_fasta_filename = os.path.join(
                out_dir, fasta_basename + '_' + str(child_fasta_count) + '.blastp-short.faa'
                )
            with open(child_fasta_filename, 'w') as child_fasta_file:
                for line in child_fasta_list:
                    child_fasta_file.write(line)
            split_fasta_fp_list.append(child_fasta_filename)
            fasta_line += max_seqs_per_process * 2
            child_fasta_count += 1

    return split_fasta_fp_list

def run_diamond(fasta_fp, diamond_fp, db_fp, taxonmap, cores):
    
    subprocess.call([
        diamond_fp,
        'blastp',
        '--threads', str(cores),
        '--db', db_fp,
        '--query', fasta_fp,
        '--taxonmap', taxonmap,
        '--more-sensitive',
        '--out', os.path.splitext(fasta_fp)[0] + '.out',
        '--outfmt', '6', 'qseqid', 'sseqid', 'evalue', 'bitscore', 'staxids', 'salltitles',
        '--evalue', '0.1',
        '--max-hsps', '1',
        '--unal', '1',
        '--block-size', '20',
        '--index-chunks', '1'
        ])

def run_blast(db_fp_list, fasta_fp_list, blastp_fp):

    # Record where the blast db's will be loaded into memory
    original_db_fp_dict = OrderedDict()
    loaded_db_fp_dict = OrderedDict()
    for db_fp in db_fp_list:
        db_name = os.path.basename(db_fp)
        original_db_fp_dict[db_name] = db_fp
        loaded_db_fp_dict[db_name] = os.path.join(
            os.path.join('/dev/shm', db_name), db_name
            )

    # Set the number of heavyweight BLAST+ processes (more memory-intensive)
    # and number of threads (less memory-intensive) for each process
    max_processes_allowed = min([cores, 16])
    num_threads = cores // max_processes_allowed

    # Make a list of all the BLAST+ commands to be executed
    commands = []
    out_fp_list = []
    for db_name, loaded_db_fp in loaded_db_fp_dict.items():
        for fasta_fp in fasta_fp_list:
            # Incorporate the name of the db into the BLAST+ output file
            out_fp = os.path.splitext(fasta_fp)[0] + '_' + db_name + '.out'
            commands.append([
                blastp_fp,
                '-task', 'blastp-short',
                '-db', loaded_db_fp,
                '-query', fasta_fp,
                '-out', out_fp,
                '-evalue', '1000000',
                '-max_target_seqs', '500',
                '-max_hsps', '1',
                '-comp_based_stats', '0',
                '-num_threads', str(num_threads),
                '-outfmt', '6 qseqid sacc evalue bitscore salltitles'
                ])
            out_fp_list.append(out_fp)

    # Run the BLAST+ commands, loading and unloading db's from mem as needed
    processes = []
    running_commands = []
    dbs_in_mem = []
    instances_started = 0
    while len(processes) > 0 or instances_started == 0:
        while len(processes) < max_processes_allowed and instances_started < len(fasta_fp_list) * len(db_fp_list):
            command = commands[instances_started]
            running_commands.append(command)
            # Unload any BLAST db's not in use
            dbs_in_use = []
            for running_command in running_commands:
                dbs_in_use.append(running_command[running_command.index('-db') + 1])
            dbs_in_use = list(set(dbs_in_use))
            for db_in_mem in dbs_in_mem:
                if db_in_mem not in dbs_in_use:
                    subprocess.call(['rm', '-r', os.path.dirname(db_in_mem)])
            # Load the BLAST db into memory if not already loaded
            loaded_db_fp = command[command.index('-db') + 1]
            if loaded_db_fp not in dbs_in_mem:
                db_name = os.path.basename(loaded_db_fp)
                subprocess.call([
                    'rm', '-rf',
                    os.path.dirname(loaded_db_fp)
                    ])
                subprocess.call([
                    'mkdir',
                    os.path.dirname(loaded_db_fp)
                    ])
                db_file_list = glob.glob(os.path.join(os.path.dirname(original_db_fp_dict[db_name]), '*'))
                subprocess.call(
                    ['cp'] + 
                    db_file_list + 
                    [os.path.dirname(loaded_db_fp)]
                    )
                dbs_in_mem.append(loaded_db_fp)

            process = subprocess.Popen(command)
            processes.append(process)
            instances_started += 1
        for i, process in enumerate(processes):
            if process.poll() is not None:
                processes.remove(process)
        time.sleep(30)

    # Unload any residual BLAST db's
    for db_in_mem in dbs_in_mem:
        subprocess.call(['rm', '-r', os.path.dirname(db_in_mem)])

    return out_fp_list

#def run_blast(fasta_fp_list, blastp_fp, db_fp):
    
#    processes = []
#    instances_started = 0
#    while len(processes) > 0 or instances_started == 0:
#        while len(processes) < cores and instances_started < len(fasta_fp_list):
#            fasta_fp = fasta_fp_list[instances_started]
#            out_fp = os.path.join(
#                os.path.dirname(fasta_fp),
#                os.path.splitext(os.path.basename(fasta_fp))[0] + '.out'
#                )
#            process = subprocess.Popen([
#                blastp_fp,
#                '-task', 'blastp-short',
#                '-db', db_fp,
#                '-query', fasta_fp,
#                '-out', out_fp,
#                '-evalue', '1000000',
#                '-max_target_seqs', '500',
#                '-max_hsps', '1',
#                '-comp_based_stats', '0',
#                '-outfmt', '6 qseqid sgi sacc evalue bitscore staxids salltitles',
#                ])
#            processes.append(process)
#            instances_started += 1
#            #print('number of instances started: ' + str(instances_started), flush=True)
#        for i, process in enumerate(processes):
#            #print('process poll for ' + str(i) + ': ' + str(process.poll()), flush=True)
#            if process.poll() is not None:
#                processes.remove(process)
#        # REMOVE
#        #pl = subprocess.Popen(['ps'], stdout=subprocess.PIPE).communicate()[0]
#        #print(pl, flush=True)
#        time.sleep(30)

    ## Script name modified
    ##blast_batch_fp = resource_filename('postnovo', 'bashscripts/blast_batch.sh')
    #blast_batch_fp = resource_filename('postnovo', 'bashscripts/blast_batch_simple.sh')
    #with open(blast_batch_fp) as blast_batch_template_file:
    #    blast_batch_template = blast_batch_template_file.read()
    #temp_blast_batch_script = blast_batch_template
    #temp_blast_batch_script = temp_blast_batch_script.replace('FASTA_FILES=', 'FASTA_FILES=({})'.format(' '.join(fasta_fp_list)))
    #temp_blast_batch_script = temp_blast_batch_script.replace('MAX_PROCESSES=', 'MAX_PROCESSES={}'.format(cores - 1))
    #temp_blast_batch_script = temp_blast_batch_script.replace('BLASTP_PATH=', 'BLASTP_PATH={}'.format(blastp_fp))
    #temp_blast_batch_script = temp_blast_batch_script.replace('DB_DIR=', 'DB_DIR={}'.format(db_fp))
    ## Script name modified
    ##temp_blast_batch_fp = os.path.join(os.path.dirname(blast_batch_fp), 'blast_batch~.sh')
    #temp_blast_batch_fp = os.path.join(os.path.dirname(blast_batch_fp), 'blast_batch_simple~.sh')
    #with open(temp_blast_batch_fp, 'w') as temp_blast_batch_file:
    #    temp_blast_batch_file.write(temp_blast_batch_script)
    #os.chmod(temp_blast_batch_fp, 0o777)
    #subprocess.call([temp_blast_batch_fp])

def get_linnean_hierarchy(high_prob_out_table, lower_prob_out_table, filtered_out_table):
    '''
    Retrieve Linnean hierarchy from NCBI taxonomy db given taxid
    '''

    taxids = filtered_out_table['taxid'].unique().tolist()
    lineage_dict = OrderedDict().fromkeys(taxids)
    one_percent_number_taxid = len(taxids) / 100 / cores
    rank_dict = OrderedDict().fromkeys(search_ranks)
    search_ranks_set = set(search_ranks)

    # Query the NCBI database to retrieve the Linnean lineage of each taxid
    ## Single process
    #lineage_lists = []
    #print_percent_progress_fn = partial(
    #    utils.print_percent_progress_singlethreaded,
    #    procedure_str='Taxonomic hierarchy recovery progress: ',
    #    one_percent_total_count=one_percent_number_taxid * cores
    #    )
    #single_var_query_entrez_taxonomy_db = partial(
    #    query_entrez_taxonomy_db,
    #    rank_dict=rank_dict, 
    #    search_ranks_set=search_ranks_set, 
    #    print_percent_progress_fn=print_percent_progress_fn
    #    )
    #for taxid in taxids:
    #    lineage_lists.append(single_var_query_entrez_taxonomy_db(taxid))

    # Multiprocess
    print_percent_progress_fn = partial(
        utils.print_percent_progress_multithreaded,
        procedure_str='Taxonomic hierarchy recovery progress: ',
        one_percent_total_count=one_percent_number_taxid,
        cores=cores
        )
    single_var_query_entrez_taxonomy_db = partial(
        query_entrez_taxonomy_db,
        rank_dict=rank_dict, 
        search_ranks_set=search_ranks_set, 
        print_percent_progress_fn=print_percent_progress_fn
        )
    multiprocessing_pool = Pool(cores)
    lineage_lists = multiprocessing_pool.map(
        single_var_query_entrez_taxonomy_db, taxids
        )
    multiprocessing_pool.close()
    multiprocessing_pool.join()

    for i, taxid in enumerate(taxids):
        lineage_dict[taxid] = lineage_lists[i]

    # Append the lineage info to each row (alignment) of the full filtered output table
    lineage_appendix_list = []
    for taxid in filtered_out_table['taxid'].tolist():
        lineage_appendix_list.append(lineage_dict[taxid])
    lineage_appendix_table = pd.DataFrame(lineage_appendix_list, columns=search_ranks)
    augmented_blast_table = pd.concat([filtered_out_table, lineage_table], axis=1)

    # Use the row ID's from the full filtered output table
    # to merge lineage info into the high and lower prob output tables
    merge_table = augmented_blast_table[['seq_number', 'hit_number'] + search_ranks]
    high_prob_out_table = high_prob_out_table.merge(merge_table, on=['seq_number', 'hit_number'])
    lower_prob_out_table = lower_prob_out_table.merge(merge_table, on=['seq_number', 'hit_number'])

    return high_prob_out_table, lower_prob_out_table, filtered_out_table

def query_entrez_taxonomy_db(taxid, rank_dict, search_ranks_set, print_percent_progress_fn):
    '''
    Given a taxid, retrieve the taxon and lineage from NCBI Taxonomy db
    '''

    print_percent_progress_fn()

    # Occasionally, Entrez will return corrupted information
    bad_lineage = True
    q_count = 0
    # I have found that the returned information is occasionally corrupted
    # Therefore, check for a bogus lineage,
    # and repeat the query up to 5 times because of this
    while bad_lineage and q_count < 5:
        q_count += 1
        # Keep trying to connect to NCBI
        while True:
            try:
                taxon_info = Entrez.read(Entrez.efetch(db='Taxonomy', id=taxid))
                break
            except:
                print('Waiting for taxid ' + str(taxid), flush=True)
                time.sleep(2)
        # Taxids can correspond to species, genera, orders, etc...
        taxon = taxon_info[0]['ScientificName']
        taxon_rank = taxon_info[0]['Rank']
        lineage_info = taxon_info[0]['LineageEx']

        taxon_ranks_set = set()
        # Record the rank corresponding to the taxid
        if taxon_rank in search_ranks_set:
            rank_dict[taxon_rank] = taxon
            taxon_ranks_set.add(taxon_rank)
        # Record the "coarser" ranks
        for entry in lineage_info:
            rank = entry['Rank']
            if rank in search_ranks_set:
                rank_dict[rank] = entry['ScientificName']
                taxon_ranks_set.add(rank)
        # Record an empty string for each "finer" rank not within the taxid lineage
        for rank in search_ranks_set.difference(taxon_ranks_set):
            rank_dict[rank] = ''

        lineage_list = [level for level in rank_dict.values()]
        # Check that the coarsest rank is a valid superkingdom (rather than a phylum, say)
        if lineage_list[-1] not in superkingdoms:
            print('Querying again due to bad lineage: ', taxid, taxon_info)
        else:
            bad_lineage = False

    return lineage_list

def find_parsimonious_taxonomy(df):

    df.set_index(id_type, inplace=True)
    list_of_taxa_assignment_rows = []
    for id in df.index.get_level_values(id_type).unique():
        id_table = df.loc[[id]]
        id_table = id_table.drop_duplicates(subset = ['taxid'])
        for rank_index, rank in enumerate(search_ranks):
            most_common_taxon_count = Counter(id_table[rank]).most_common(1)[0]
            if most_common_taxon_count[0] != '' and pd.notnull(most_common_taxon_count[0]):
                if most_common_taxon_count[1] >= taxon_assignment_threshold * len(id_table):
                    id_table.reset_index(inplace = True)
                    try:
                        representative_row = id_table.ix[
                            id_table[
                                id_table[rank] == most_common_taxon_count[0]
                                ][rank].first_valid_index()
                            ]
                    except:
                        print('id: ' + str(id), flush=True)
                        print('rank: ' + str(rank), flush=True)
                        print('is pd.null:')
                        print(pd.isnull(most_common_taxon_count[0]))
                        sys.exit()
                    list_of_taxa_assignment_rows.append(
                        [id] + \
                            [representative_row['seq']] + \
                            [representative_row['precursor_mass']] + \
                            [representative_row['seq_score']] + \
                            [representative_row['seq_origin']] + \
                            rank_index * ['N/A'] + \
                            representative_row[rank:].tolist()
                        )
                    break
        else:
            representative_row = id_table.iloc[0]
            list_of_taxa_assignment_rows.append(
                [id] + \
                    [representative_row['seq']] + \
                    [representative_row['precursor_mass']] + \
                    [representative_row['seq_score']] + \
                    [representative_row['seq_origin']] + \
                    len(search_ranks) * ['N/A']
                )

    taxa_assignment_table = pd.DataFrame(
        list_of_taxa_assignment_rows,
        columns=['scan_list', 'seq', 'precursor_mass', 'seq_score', 'seq_origin'] + search_ranks)

    taxa_count_table = pd.DataFrame()
    for rank in search_ranks:
        taxa_counts = Counter(taxa_assignment_table[rank])
        taxa_count_table = pd.concat(
            [taxa_count_table,
             pd.Series([taxon for taxon in taxa_counts.keys()], name = rank + ' taxa'),
             pd.Series([count for count in taxa_counts.values()], name = rank + ' counts')],
             axis = 1
             )

    df.reset_index(inplace=True)

    return taxa_assignment_table, taxa_count_table

def screen_taxonomic_profile(high_prob_taxa_assign_df, high_prob_taxa_count_df, low_prob_df, high_prob_df):
    '''
    Screen the low-prob results for those in the high-prob taxonomic profile
    '''

    gold_taxa_profile_dict = OrderedDict()
    silver_taxa_profile_dict = OrderedDict()
    for level in ['family', 'genus', 'species']:
        name_list_raw = high_prob_taxa_count_df[level + ' taxa'].tolist()
        count_list_raw = high_prob_taxa_count_df[level + ' counts'].tolist()
        name_list = []
        count_list = []
        for i in range(len(name_list_raw)):
            if pd.notnull(name_list_raw[i]):
                name_list.append(name_list_raw[i])
                count_list.append(count_list_raw[i])
        count_dict = {name_list[i]: count_list[i] for i in range(len(name_list))}
        # Taxa in profile must not be singletons
        gold_taxa_profile_dict[level] = [i for i in count_dict if count_dict[i] > 5]
        silver_taxa_profile_dict[level] = [i for i in count_dict if 5 >= count_dict[i] > 1]

    is_in_profile_list = [0 for i in range(len(low_prob_df))]
    for level in ['family', 'genus', 'species']:
        gold_taxa_profile_list = gold_taxa_profile_dict[level]
        silver_taxa_profile_list = silver_taxa_profile_dict[level]
        low_prob_taxa_list = low_prob_df[level].tolist()
        for i, low_prob_taxon in enumerate(low_prob_taxa_list):
            if not is_in_profile_list[i]:
                if low_prob_taxon in gold_taxa_profile_list:
                    is_in_profile_list[i] = 1
                elif low_prob_taxon in silver_taxa_profile_list:
                    is_in_profile_list[i] = 2
    low_prob_df['is_in_profile'] = is_in_profile_list

    low_prob_profile_df = low_prob_df[low_prob_df['is_in_profile'] > 0]
    profile_df = pd.concat([high_prob_df, low_prob_profile_df], ignore_index=True)

    return profile_df, low_prob_profile_df

def sample_hits(parent_scan_group, sample_size = 10):

    # Consider hits within 2 bits of the top hit in the profile group
    scan_group = parent_scan_group[
        parent_scan_group['bitscore'] >= (parent_scan_group['bitscore'].max() - 2)
        ]
    # If the taxonomic profile lacks confidence, consider the full table
    if 1 not in scan_group['is_in_profile'].values:
        scan_group = parent_scan_group

    group_size = len(scan_group)
    if group_size <= 10:
        return scan_group
    else:
        group_rows = list(range(group_size))
        div, mod = divmod(len(group_rows), sample_size)
        sample_rows = [group_rows[i * div + min(i, mod)] for i in range(sample_size)]
        return scan_group.iloc[sample_rows]

def make_full_hit_seq_fasta(sampled_df, generic_emapper_fasta_fp):
    
    accession_list = sampled_df['accession'].tolist()
    one_percent_number_subject_seqs = len(accession_list) / 100 / cores

    # Multiprocess
    multiprocessing_pool = Pool(cores)
    print_percent_progress_fn = partial(
        utils.print_percent_progress_multithreaded,
        procedure_str = 'Full subject sequence recovery progress: ',
        one_percent_total_count = one_percent_number_subject_seqs,
        cores = cores
        )
    single_var_query_ncbi_protein = partial(
        query_ncbi_protein,
        print_percent_progress_fn = print_percent_progress_fn
        )
    full_hit_seq_list = multiprocessing_pool.map(single_var_query_ncbi_protein, accession_list)
    multiprocessing_pool.close()
    multiprocessing_pool.join()

    sampled_df[id_type] = sampled_df[id_type].apply(str)
    sampled_df['hit'] = sampled_df['hit'].apply(str)
    sampled_df['full seq'] = full_hit_seq_list

    eggnog_fasta_path_list = []
    for superkingdom in superkingdoms:
        superkingdom_df = sampled_df[sampled_df['superkingdom'] == superkingdom]
        # Only write to file if there is information for the superkingdom
        if len(superkingdom_df) > 0:
            superkingdom_id_list = superkingdom_df[id_type].tolist()
            superkingdom_hit_list = superkingdom_df['hit'].tolist()
            superkingdom_seq_list = superkingdom_df['full seq'].tolist()
            superkingdom_header_list = [
                '>' + '(' + id_type + ')' + superkingdom_id_list[i] + '(hit)' + superkingdom_hit_list[i]
                for i in range(len(superkingdom_id_list))
                ]
            superkingdom_header_seq_dict = {
                superkingdom_header_list[i]: superkingdom_seq_list[i]
                for i in range(len(superkingdom_header_list))
                }

            # online emapper with hmmr requires <=5000 seqs/file
            if local_eggnog:
                hmmr_seq_count_limit = 1000000000
            else:
                hmmr_seq_count_limit = 5000
            dirname = os.path.dirname(generic_emapper_fasta_fp)
            basename = os.path.splitext(os.path.basename(generic_emapper_fasta_fp))[0]

            previous_last_row = 0
            for i in range(len(superkingdom_header_list) // hmmr_seq_count_limit + 1):
                superkingdom_write_path = os.path.join(
                    dirname, basename + '.' + superkingdom.lower() + '_' + str(i) + '.faa'
                    )
                eggnog_fasta_path_list.append(basename + '.' + superkingdom.lower() + '_' + str(i) + '.faa')
                with open(superkingdom_write_path, 'w') as f:
                    for j, header in enumerate(superkingdom_header_list[previous_last_row:]):
                        f.write(header + '\n')
                        f.write(superkingdom_header_seq_dict[header] + '\n')
                        if j+1 == hmmr_seq_count_limit:
                            previous_last_row = (j+1) * (i+1)
                            break

    return eggnog_fasta_path_list
    
def query_ncbi_protein(accession, print_percent_progress_fn):

    print_percent_progress_fn()

    while True:
        try:
            full_seq = Entrez.read(
                Entrez.efetch(db='Protein', id=accession, retmode='xml')
                )[0]['GBSeq_sequence']
            break
        except:
            print('Waiting for ' + accession, flush=True)
            time.sleep(2)

    return full_seq

def parse_eggnog_mapper_output(eggnog_out_fp_list):

    eggnog_df = pd.DataFrame(columns=eggnog_output_headers)
    for out_fp in eggnog_out_fp_list:
        if os.path.exists(out_fp):
            if os.stat(out_fp).st_size > 0:
                eggnog_output_df = pd.read_csv(out_fp, sep='\t', header=None, names=eggnog_output_headers, comment='#')
                eggnog_df = pd.concat([eggnog_df, eggnog_output_df], ignore_index=True)

    # Split header into two cols for scan lists and hits
    query_list = eggnog_df['query'].tolist()
    query_list = [query.split('(' + id_type + ')')[1] for query in query_list]
    temp_list_of_lists = [query.split('(hit)') for query in query_list]
    id_list = [temp_list[0] for temp_list in temp_list_of_lists]
    hit_list = [temp_list[1] for temp_list in temp_list_of_lists]
    eggnog_df.drop('query', axis=1, inplace=True)
    eggnog_df[id_type] = id_list
    eggnog_df['hit'] = hit_list

    return eggnog_df

def find_coherent_hits(eggnog_df, sampled_df):

    # Need to account for queries that do not have an eggnog annotation
    # Need a table of all id + hit
    # Groupby id, make sorted list of hit
    # Sorted list of hits for id must match that from eggnog annotations to call annotations conserved

    conserv_func_df_list = []
    eggnog_df_groups = eggnog_df.groupby(id_type)
    sampled_df_groups = sampled_df.groupby(id_type)
    for id in list(set(eggnog_df[id_type])):
        eggnog_df_group = eggnog_df_groups.get_group(id)
        sampled_df_group = sampled_df_groups.get_group(id)
        # If all annotated seqs have the same description string
        if (eggnog_df_group['eggnog hmm desc'] == eggnog_df_group['eggnog hmm desc'].iloc[0]).all():
            # If all the queries (sampled blast hits) are present in the annotation file
            if len(eggnog_df_group) == len(sampled_df_group):
                conserv_func_df_list.append(eggnog_df_group)
    conserv_func_df = pd.concat(conserv_func_df_list, ignore_index=True)
    return conserv_func_df

def condense_hits(conserv_func_df, low_prob_profile_df, high_prob_taxa_assign_df):

    # Condense rows of each group
    conserv_func_df = conserv_func_df.fillna('')
    id_set_list = list(set(conserv_func_df['scan_list'].tolist()))
    parsed_conserv_func_df_cols_dict = OrderedDict(
        [(header, []) for header in parsed_conserv_func_df_headers_list])
    parsed_conserv_func_df = pd.DataFrame()
    parsed_conserv_func_df[id_type] = id_set_list
    conserv_func_df_groups = conserv_func_df.groupby(id_type)
    # Loop through each group
    for id in id_set_list:
        conserv_func_df_group = conserv_func_df_groups.get_group(id)
        # Go through each col in group
        # seed ortholog: set intersection
        parsed_conserv_func_df_cols_dict['seed ortholog'].append(
            ','.join(list(set(conserv_func_df_group['seed ortholog']))))
        # evalue: ignore
        # score: ignore
        # predicted name: set intersection
        parsed_conserv_func_df_cols_dict['predicted name'].append(
            ','.join(list(set(conserv_func_df_group['predicted name']))))
        # go terms: set intersection
        list_of_lists = conserv_func_df_group['go terms'].apply(lambda x: x.split(',')).tolist()
        l = [i for sublist in list_of_lists for i in sublist]
        parsed_conserv_func_df_cols_dict['go terms'].append(','.join(list(set(l))))
        # kegg pathways: set intersection
        list_of_lists = conserv_func_df_group['kegg pathways'].apply(lambda x: x.split(',')).tolist()
        l = [i for sublist in list_of_lists for i in sublist]
        parsed_conserv_func_df_cols_dict['kegg pathways'].append(
            ','.join(list(set(l))))
        # tax scope: set intersection
        parsed_conserv_func_df_cols_dict['tax scope'].append(
            ','.join(list(set(conserv_func_df_group['tax scope']))))
        # eggnog ogs: set intersection
        list_of_lists = conserv_func_df_group['eggnog ogs'].apply(lambda x: x.split(',')).tolist()
        l = [i for sublist in list_of_lists for i in sublist]
        parsed_conserv_func_df_cols_dict['eggnog ogs'].append(
            ','.join(list(set(l))))
        # best og: set intersection of first substring before |
        parsed_conserv_func_df_cols_dict['best og'].append(
            ','.join(list(set(
                conserv_func_df_group['best og'].apply(lambda x: x.split('|')[0])))))
        # cog cat: set intersection
        parsed_conserv_func_df_cols_dict['cog cat'].append(
            ','.join(list(set(conserv_func_df_group['cog cat']))))
        # eggnog hmm desc: first row (all rows the same)
        parsed_conserv_func_df_cols_dict['eggnog hmm desc'].append(
            conserv_func_df_group['eggnog hmm desc'].iloc[0])
        # scan_list: already placed in parsed_conserv_func_df
        # hit: ignore
    for header, col in parsed_conserv_func_df_cols_dict.items():
        parsed_conserv_func_df[header] = col

    low_prob_profile_df.set_index(id_type, inplace=True)
    id_set_list = set(id_set_list).intersection(set(low_prob_profile_df.index.tolist()))
    low_prob_profile_df = low_prob_profile_df.loc[id_set_list]
    low_prob_profile_df.reset_index(inplace=True)
    low_prob_profile_df = low_prob_profile_df.drop('is_in_profile', axis=1)
    low_prob_profile_taxa_assign_df, low_prob_profile_taxa_count_df = \
        find_parsimonious_taxonomy(low_prob_profile_df)

    if 'hit' in high_prob_taxa_assign_df.columns:
        high_prob_taxa_assign_df.drop('hit', axis=1, inplace=True)

    profile_taxa_assign_df = pd.concat([high_prob_taxa_assign_df, low_prob_profile_taxa_assign_df], ignore_index=True)

    reported_df = parsed_conserv_func_df.merge(profile_taxa_assign_df, how='inner', on=id_type)
    reported_df.fillna('', inplace=True)
    reported_df.replace('N/A', '', inplace=True)
    # Put double quotes around scan_list values
    return reported_df

if __name__ == '__main__':
    main()