import argparse
import copy
import glob
import numpy as np
import os
import pandas as pd
import pkg_resources
import pickle as pkl
import re
import subprocess
import sys
import time

from Bio import Entrez
from collections import OrderedDict, Counter
from functools import partial
from multiprocessing import Pool, cpu_count
from pkg_resources import resource_filename

if 'postnovo' in sys.modules:
    import postnovo.config as config
    import postnovo.utils as utils
else:
    import config
    import utils

local_eggnog = False
fasta_input_fp = ''
out_dir = ''

min_seq_len_for_diamond = 31

Entrez.email = 'samuelmiller@uchicago.edu'
cores = 1

# Output field names specified in BLAST+
blast_out_hdr = [
    'qseqid',
    'accession',
    'evalue',
    'bitscore', 
    'stitle'
    ]
# Output field names specified in DIAMOND
diamond_out_hdr = [
    'qseqid',
    'sseqid',
    'evalue',
    'bitscore',
    'staxids', 
    'stitle'
    ]
# Query seq info, as reported in postnovo_seqs_info.tsv
seq_info_hdr = [
    'seq_number',
    'seq',
    'scan_list',
    'score',
    'best_predicts_from',
    'also_contains_predicts_from'
    ]
# Column names of merged DIAMOND and BLAST output
align_out_hdr = seq_info_hdr + [
    'hit_number',
    'evalue',
    'bitscore',
    'taxid',
    'stitle'
    ]
# scan list, score, and best prediction and subseq origin list
max_float = np.finfo('f').max
max_int = np.finfo('d').max

superkingdoms = [
    'Archaea', 
    'Bacteria',
    'Eukaryota',
    'Viruses'
    ]
search_ranks = [
    'species', 
    'genus', 
    'family', 
    'order', 
    'class', 
    'phylum', 
    'superkingdom'
    ]
profile_ranks = [
    'species',
    'genus', 
    'family'
    ]
taxon_assignment_threshold = 0.9

emapper_abbr_dict = {
    'archaea': 'arch',
    'bacteria': 'bact', 
    'eukaryota': 'euk', 
    'viruses': 'viruses'
    }

eggnog_output_headers = [
    'query', 
    'seed ortholog', 
    'evalue', 
    'score', 
    'predicted name',
    'go terms', 
    'kegg pathways', 
    'tax scope', 
    'eggnog ogs', 
    'best og',
    'cog cat', 
    'eggnog hmm desc'
    ]

parsed_conserv_func_df_headers_list = [
    'seed ortholog', 
    'predicted name', 
    'go terms', 
    'kegg pathways',
    'tax scope', 
    'eggnog ogs', 
    'best og', 
    'cog cat', 
    'eggnog hmm desc'
    ]

# The e-value cutoff for "confident," "non-random" alignments
confident_evalue = 0.1

def main(test_argv=None):
    global local_eggnog, out_dir

    test_argv = []
    if 'test_argv' in locals():
        args = parse_args(test_argv)
    else:
        args = parse_args()

    if args.command == 'make_emapper_input':
        if args.local_eggnog:
            local_eggnog = True
        make_emapper_input(args)
    elif args.command == 'analyze_eggnog_output':
        out_dir = args.iodir

        # Load files generated by make_emapper_input
        sampled_df = pd.read_csv(
            os.path.join(out_dir, 'sampled_df.csv')
            )
        low_prob_profile_df = pd.read_csv(
            os.path.join(out_dir, 'low_prob_profile_df.csv')
            )
        high_prob_taxa_assign_df = pd.read_csv(
            os.path.join(out_dir, 'high_prob_taxa_assign_df.csv')
            )
        with open(os.path.join(out_dir, 'eggnog_fasta_path_list.pkl'), 'rb') as f:
            eggnog_fasta_path_list = pkl.load(f)

        analyze_eggnog_output(
            args, sampled_df, low_prob_profile_df, high_prob_taxa_assign_df, eggnog_fasta_path_list
            )

def make_emapper_input(args):
    
    global fasta_input_fp, out_dir
    fasta_list = open(args.postnovo_seqs, 'r').readlines()
    fasta_input_fp = args.postnovo_seqs

    out_dir = os.path.dirname(fasta_input_fp)

    # If BLAST+ and DIAMOND have already been run,
    if args.redo_without_search:

        out_dir_files = [f for f in os.listdir(out_dir) if os.path.isfile(os.path.join(out_dir, f))]
        diamond_out_fp = ''
        split_blast_out_fp_list = []
        fasta_basename = os.path.splitext(os.path.basename(fasta_input_fp))[0]
        for f in out_dir_files:
            if fasta_basename + '.diamond.out' in f:
                if diamond_out_fp != '':
                    raise Exception('DIAMOND output of the name ' + os.path.splitext(fasta_input_fp)[0] + '.diamond.out already exists')
            elif fasta_basename in f and 'blastp-short.out' in f and os.path.splitext(f)[1] == '.out':
                split_blast_out_fp_list.append(os.path.join(out_dir, f))

    else:

        # Check that the input fasta file looks like a fasta file
        parent_fasta_size = len(fasta_list) / 2
        if parent_fasta_size % int(parent_fasta_size) > 0:
            raise ValueError('The fasta input must have an even number of lines.')

        # Separate fasta entries into longer seqs for DIAMOND and shorter seqs for BLAST
        diamond_fasta_list = []
        blast_fasta_list = []
        entry = []
        for line in fasta_list:
            entry.append(line)
            if line[0] != '>':
                if len(line.rstrip()) >= min_seq_len_for_diamond:
                    diamond_fasta_list += entry
                else:
                    blast_fasta_list += entry
                entry = []

        # Write fasta file for DIAMOND
        diamond_fasta_fp = os.path.splitext(fasta_input_fp)[0] + '.diamond.faa'
        with open(diamond_fasta_fp, 'w') as handle:
            for line in diamond_fasta_list:
                handle.write(line)

        # To run BLAST+ on multiple CPUs, split the fasta input into multiple files
        split_blast_fasta_fp_list = split_blast_fasta(blast_fasta_list, args.max_seqs_per_blast_instance)

        #run_diamond(diamond_fasta_fp, args.diamond, args.diamond_db, args.taxonmap, args.cores)
        split_blast_out_fp_list = run_blast(args.blast_dbs, split_blast_fasta_fp_list, args.blastp)        

        sys.exit()

    # Make a table of sequence information to add to the alignment output
    seq_info_table = make_info_table(fasta_input_fp, args.postnovo_seqs_info)

    # Load the BLAST output from alignments against each db
    db_names = [os.path.basename(blast_db_fp) for blast_db_fp in args.blast_dbs]
    blast_out_dict = OrderedDict().fromkeys(db_names)
    for db_name in db_names:
        blast_out_table = pd.DataFrame(columns=blast_out_hdr)
        # Piece the BLAST table together from its parts
        for partial_blast_out_fp in split_blast_out_fp_list:
            partial_blast_out_table = pd.read_csv(
                partial_blast_out_fp, sep='\t', header=None, names=blast_out_hdr
                )
            blast_out_table = pd.concat(
                [blast_out_table, partial_blast_out_table], ignore_index=True
                )

        # Reorganize the qseqid column into the seq_number column
        blast_out_table['seq_number'] = blast_out_table['qseqid'].apply(
            lambda x: int(x.replace('(seq_number)', ''))
            )
        blast_out_table.drop('qseqid', inplace=True)
        # Add a hit_number column to keep track of the alignment rank
        blast_out_table['hit_number'] = make_hit_number_list(blast_out_table['seq_number'].tolist())

        # Add seq info to the blast output table
        blast_out_table = blast_out_table.merge(seq_info_table, on=['seq_number'])

        # For debugging purposes, write the current db BLAST table to file
        if args.intermediate_files:
            blast_out_table.to_csv(
                os.path.join(out_dir, db_name + '_blast_out.csv'), index=False
                )

        blast_out_dict[db_name] = blast_out_table

    # Compare results from multiple databases
    if len(blast_out_dict) == 1:
        blast_out_table = list(blast_out_dict.values())[0]
    else:
        blast_out_table = refine_blast_table(blast_out_dict)

    # Unzipped taxonmap file should be provided in the same dir as zipped file
    taxonmap_fp = os.path.splitext(args.taxonmap)[0]
    # List the accessions in the (refined) BLAST results
    out_accessions = list(set(blast_out_table['accession'].tolist()))
    taxid_dict = OrderedDict().fromkeys(out_accessions)
    with open(args.taxonmap) as handle:
        # Search the file for accessions until all have been found
        for line in handle:
            # The first field of accession2taxid is the accession
            accession = line.split('\t')[0]
            if accession in out_accessions:
                # The third field is the taxid
                taxid_dict[accession] = line.split('\t')[2]
                out_accessions.remove(accession)
                if not out_accessions:
                    break
    # Throw an exception if accessions remain unassigned
    if out_accessions:
        raise Exception(
            'The following accessions were not found in ' + 
            os.path.basename(taxonmap_fp) + ': ' +
            ', '.join(out_accessions)
            )
    taxid_list = []
    for accession in blast_out_table['accession'].tolist():
        taxid_list.append(taxid_dict[accession])
    blast_out_table['taxid'] = taxid_list
    # Rearrange the cols in a predictable order
    blast_out_table = blast_out_table[align_out_hdr]

    # Load the DIAMOND output
    diamond_out_table = pd.read_csv(
        os.path.splitext(fasta_input_fp)[0] + '.diamond.out', sep='\t', header=None, names=align_out_hdr
        )
    # Remove seqs without an alignment
    diamond_out_table = diamond_out_table[diamond_out_table['accession'] != '*']
    diamond_out_table['seq_number'] = diamond_out_table['qseqid'].apply(
        lambda x: x.replace('(seq_number)', '')
        )
    # Remove the version number from the accession to make it consistent with BLAST
    diamond_out_table['accession'] = diamond_out_table['sseqid'].apply(lambda x: x.split('.')[0])
    diamond_out_table.drop('sseqid', inplace=True)
    # Add a hit number column
    diamond_out_table['hit_number'] = make_hit_number_list(diamond_out_table['seq_number'].tolist())
    # Rename 'staxids' to 'taxid' to be consistent with BLAST
    diamond_out_table['taxid'] = diamond_out_table['staxids']
    diamond_out_table.drop('staxids', inplace=True)
    # Remove the accession.version substring from stitle to make it consistent with BLAST
    diamond_out_table['stitle'] = diamond_out_table['stitle'].apply(lambda x: x[x.index(' ') + 1:])
    
    # Add seq info to the DIAMOND output table
    diamond_out_table = diamond_out_table.merge(seq_info_table, on=['seq_number'])
    # Rearrange the cols in a predictable order
    diamond_out_table = diamond_out_table[align_out_hdr]

    # Combine DIAMOND and BLAST output
    align_out_table = pd.concat([diamond_out_table, blast_out_table], ignore_index=True)
    align_out_table.sort_values(['seq_number', 'hit'], inplace=True)

    # REMOVE
    # For debugging, report any seqs that return multiple taxids:
    # these should be separated by a semicolon (according to DIAMOND manual),
    # and the column datatype is therefore an object rather than numeric
    if not pd.api.types.is_numeric_dtype(align_out_table['taxid']):
        print(align_out_table[align_out_table['taxid'].apply(lambda x: ';' in x)], flush=True)

    # For debugging purposes, write the alignment output table to file
    if args.intermediate_files:
        align_out_table.to_csv(
            os.path.join(out_dir, 'align_out.csv'), index=False
            )

    # Test the data types of the columns to avoid unexpected surprises
    assert pd.api.types.is_numeric_dtype(align_out_table['seq_number'])
    assert pd.api.types.is_numeric_dtype(align_out_table['hit_number'])
    assert pd.api.types.is_object_dtype(align_out_table['accession'])
    assert pd.api.types.is_numeric_dtype(align_out_table['evalue'])
    assert pd.api.types.is_numeric_dtype(align_out_table['bitscore'])
    assert pd.api.types.is_numeric_dtype(align_out_table['taxid'])
    assert pd.api.types.is_object_dtype(align_out_table['stitle'])

    # Filter the alignment output table by evalue,
    # sorting into high- and low-probability sub-tables
    high_prob_out_table, lower_prob_out_table, filtered_out_table = filter_align_out_table(align_out_table)

    if args.intermediate_files:
        high_prob_out_table.to_csv(
            os.path.join(out_dir, 'high_prob_out.csv'), index=False
            )
        lower_prob_out_table.to_csv(
            os.path.join(out_dir, 'lower_prob_out.csv'), index=False
            )
        filtered_out_table.to_csv(
            os.path.join(out_dir, 'filtered_out.csv'), index=False
            )

    high_prob_out_table, lower_prob_out_table, filtered_out_table = get_linnean_hierarchy(
        high_prob_out_table, lower_prob_out_table, filtered_out_table
        )

    if args.intermediate_files:
        high_prob_out_table.to_csv(
            os.path.join(out_dir, 'high_prob_out_with_lineages.csv'), index=False
            )
        lower_prob_out_table.to_csv(
            os.path.join(out_dir, 'lower_prob_out_with_lineages.csv'), index=False
            )
        filtered_out_table.to_csv(
            os.path.join(out_dir, 'filtered_out_table_with_lineages.csv'), index=False
            )

    # Determine the "parsimonious" taxonomic classification that encompasses a query seq's hits
    high_prob_taxa_table, high_prob_taxa_count_table = assign_taxonomy(high_prob_out_table)
    # The high probability taxonomic assignment table is written to file,
    # as it is later used in blastseqs.analyze_emapper_output
    high_prob_taxa_table.to_csv(
        os.path.join(out_dir, 'high_prob_taxa_table.csv'), index=False
        )

    if args.intermediate_files:
        high_prob_taxa_count_table.to_csv(
            os.path.join(out_dir, 'high_prob_taxa_count_table.csv'), index=False
            )

    # Filter the lower prob hits by the taxonomic profile of the high prob results
    filtered_out_table, low_prob_out_table = filter_lower_prob_taxa(
        high_prob_taxa_table, high_prob_taxa_count_table, high_prob_out_table, lower_prob_out_table
        )

    if args.intermediate_files:
        filtered_out_table.to_csv(
            os.path.join(out_dir, 'filtered_out_by_taxa_profile.csv'), index=False
            )
    lower_prob_out_table.to_csv(
        os.path.join(out_dir, 'lower_prob_out_filtered_by_taxa_profile.csv'), index=False
        )

    # Multiple hits per query seq can be retained at this stage
    # Each set of hits must have uniform functional annotations
    # When more than 10 hits are considered, evenly sample 10 hits
    sampled_out_table = filtered_out_table.groupby('seq_number').apply(sample_hits)
    sampled_out_table.to_csv(
        os.path.join(out_dir, 'sampled_out_table.csv'), index=False
        )

    # Make fasta files to be used as the input for eggNOG-mapper functional annotation
    # eggNOG-mapper needs long sequences for functional annotation,
    # so retrieve the full subject sequence for each hit
    sampled_out_table['full subject seq'] = get_full_subject_seqs(sampled_out_table['accession'].tolist())
    # There are "superkingdom"-level eggNOG databases,
    # so make multiple files containing seqs of each superkingdom affiliation
    fasta_filename_prefix = os.path.splitext(os.path.basename(fasta_input_fp))[0]
    eggnog_fasta_paths = write_eggnog_fasta_files(sampled_out_table, out_dir, fasta_filename_prefix)
    with open(os.path.join(out_dir, 'eggnog_fasta_path_list.pkl'), 'wb') as handle:
        pkl.dump(eggnog_fasta_paths, handle, 2)

    return sampled_out_table, lower_prob_out_table, high_prob_taxa_table, eggnog_fasta_paths

def make_hit_number_list(id_list):

    hit_number = 0
    hit_number_list = [0]
    last_id = 0
    for id in id_list[1:]:
        if id != last_id:
            hit_number = 0
            hit_number_list.append(hit_number)
            last_id = id
        else:
            hit_number += 1
            hit_number_list.append(hit_number)
    blast_out_table['hit_number'] = hit_number_list

    return hit_number_list

def make_info_table(fasta_input_fp, info_fp):

    # Retrieve the query sequences from the postnovo fasta file
    with open(fasta_input_fp) as handle:
        fasta_input_list = handle.readlines()
    query_seqs = [seq.rstrip() for seq in fasta_input_list[1::2]]

    # Retrieve scan list, score, and best prediction and subseq origin lists
    # for each seq from postnovo_seqs_info.tsv
    seq_info_table = pd.read_csv(info_fp, sep='\t', header=0)
    seq_info_table['seq'] = query_seqs

    return seq_info_table

def refine_blast_table(blast_out_dict):

    # Make a list of all the seq numbers in the BLAST output
    # (This ignores seqs that are not reported at all
    # when the alignment has an e-value greater than the threshold.)
    # Create a groupby (seq number) object for each blast table
    seq_numbers = []
    groupby_dict = OrderedDict().fromkeys(blast_out_dict)
    for db_name, blast_out_table in blast_out_dict.items():
        seq_numbers += blast_out_table['seq_number'].tolist()
        groupby_dict[db_name] = blast_out_table.groupby('seq_number')
    seq_number_set = set(seq_numbers)
    
    # Loop through each seq to consider alignments against the different db's
    # First, screen by a "non-random alignment" e-value cutoff
    # If a seq has one or more alignments below this cutoff,
    # take the results from the database with the lowest e-value
    # Second, if no seq has an alignment below this cutoff,
    # take the results from the first (most general) database
    refined_blast_results = []
    for seq_number in seq_number_set:
        lowest_evalue = max_float
        optimum_db_name = None
        for db_name, groupby_table in groupby_dict.items():
            # The table may not have an alignment for the seq
            try:
                evalue = groupby_table.get_group(seq_number)['evalue'].min()
                if evalue < lowest_evalue:
                    lowest_evalue = evalue
                    optimum_db = db_name
            except KeyError:
                pass
            # Store the db results in a list to be concatenated into a full table
            if lowest_evalue < confident_evalue:
                refined_blast_results.append(
                    groupby_dict[optimum_db].get_group(seq_number)
                    )
            else:
                # Find the most general db (db's should be in order of generality)
                # containing an alignment for any seq that does not meet the e-value cutoff
                for _, groupby_table1 in groupby_dict.items():
                    try:
                        refined_blast_results.append(
                            groupby_table1.get_group(seq_number)
                            )
                        break
                    except KeyError:
                        pass
    refined_blast_table = pd.concat(refined_blast_results, ignore_index=True)

    return refined_blast_table

def filter_align_out_table(align_out_table):

    seq_numbers = sorted(list(set(align_out_table['seq_number'].tolist())))
    high_prob_subtables = []
    lower_prob_subtables = []
    all_filtered_subtables = []
    groupby_table = align_out_table.groupby('seq_number')
    for seq_number in seq_numbers:
        subtable = groupby_table.get_group(seq_number)
        # For query seqs with an alignment meeting the e-value cutoff,
        # retain all alignments sharing the best bit score
        if subtable['evalue'].min() <= confident_evalue:
            high_prob_subtable = subtable[subtable['bitscore'] == subtable['bitscore'].max()]
            high_prob_subtables.append(high_prob_subtable)
            all_filtered_subtables.append(high_prob_subtable)
        # Otherwise retain all reported alignments (up to 500 in the case of BLAST+)
        else:
            lower_prob_subtables.append(subtable)
            all_filtered_subtables.append(subtable)

    high_prob_out_table = pd.concat(high_prob_subtables, ignore_index=True)
    lower_prob_out_table = pd.concat(lower_prob_subtables, ignore_index=True)
    filtered_out_table = pd.concat(all_filtered_subtables, ignore_index=True)

    return high_prob_out_table, lower_prob_out_table, filtered_out_table

def analyze_eggnog_output(args, sampled_df, low_prob_profile_df, high_prob_taxa_assign_df, eggnog_fasta_path_list):

    # Load as dataframe
    # Assign predefined column names
    # Concat annotation dfs
    eggnog_out_fp_list = [
        os.path.join(args.iodir, fasta + '.emapper.annotations') for fasta in eggnog_fasta_path_list
        ]

    eggnog_df = parse_eggnog_mapper_output(eggnog_out_fp_list)
    conserv_func_df = find_coherent_hits(eggnog_df, sampled_df)
    reported_df = condense_hits(conserv_func_df, low_prob_profile_df, high_prob_taxa_assign_df)
    reported_df.to_csv(
        os.path.join(args.iodir, 'reported_df.tsv'),
        sep='\t',
        index=False
        )

def parse_args(test_argv = None):

    parser = argparse.ArgumentParser(
        description = 'BLAST+ and taxonomic identification of peptide sequences'
        )
    subparsers = parser.add_subparsers(dest='command')

    parser_make_emapper_input = subparsers.add_parser(
        'make_emapper_input',
        help=('make fasta files for eggNOG-mapper input: '
              'step 1 of remote_eggnog procedure')
        )

    make_emapper_input_seq_source_group = parser_make_emapper_input.add_mutually_exclusive_group()
    make_emapper_input_seq_source_group.add_argument(
        '--postnovo_seqs',
        help='path to postnovo_seqs.faa, the fasta file of query sequences generated by postnovo'
        )
    parser_make_emapper_input.add_argument(
        '--postnovo_seqs_info',
        help='path to postnovo_seqs_info.tsv, produced at the same time as postnovo_seqs.faa'
        )

    parser_make_emapper_input.add_argument(
        '--diamond',
        help=(
            'path to DIAMOND executable: '
            'DIAMOND is used for fast alignment of query sequences longer than 30 AAs'
            )
        )
    parser_make_emapper_input.add_argument(
        '--diamond_db',
        help=(
            'DIAMOND database path as it would be specified in a DIAMOND search, '
            'e.g., /home/samuelmiller/diamond_db/refseq/refseq.dmnd'
            )
        )
    parser_make_emapper_input.add_argument(
        '--blastp',
        help=(
            'path to blastp executable: '
            'BLAST+ blastp-short is used for alignment of query sequences shorter than 30 AAs'
            )
        )
    parser_make_emapper_input.add_argument(
        '--blast_dbs',
        nargs='+',
        help=(
            'BLAST database paths as they would be specified in a BLAST+ search, '
            'e.g., /home/samuelmiller/blast_db/refseq_protein/refseq_protein '
            'If multiple databases are used, '
            'place in order of most general to specific, '
            'e.g., full RefSeq precedes bacterial RefSeq. '
            'Databases should be constructed from fasta files with makeblastdb '
            'after converting all occurrences of Ile to Leu. '
            'The database (not fasta) files should be in their own directory.'
            )
        )
    parser_make_emapper_input.add_argument(
        '--max_seqs_per_blast_instance',
        type=int,
        default=1000,
        help='maximum number of query seqs per BLAST+ instance'
        )
    parser_make_emapper_input.add_argument(
        '--taxonmap',
        help=(
            'Filepath to prot.accession2taxid.gz (from NCBI) '
            'for retrieving taxonomy from accession IDs: '
            'both zipped and unzipped files should be present in the directory '
            )
        )
    parser_make_emapper_input.add_argument(
        '--local_eggnog',
        default=False,
        action='store_true',
        help='flag to create files for local eggNOG'
        )
    parser_make_emapper_input.add_argument(
        '--cores',
        type=int,
        default=1,
        help='number of cores to use'
        )
    parser_make_emapper_input.add_argument(
        '--intermediate_files',
        default=False,
        action='store_true',
        help='flag to write intermediate files for debugging'
        )
    parser_make_emapper_input.add_argument(
        '--redo_without_search',
        default=False,
        action='store_true',
        help='flag to make fasta files after DIAMOND and BLAST output have been generated'
        )

    parser_analyze_emapper_output = subparsers.add_parser(
        'analyze_emapper_output',
        help='analyze eggNOG-mapper + HMMR annotation output'
        )
    parser_analyze_emapper_output.add_argument(
        '--iodir',
        help=('directory containing sampled_df.csv, '
              'low_prob_profile_df.csv, '
              'high_prob_taxa_assign_df.csv, '
              'and eggnog_fasta_path_list.pkl (all produced by make_emapper_input step), '
              'and eggNOG-mapper annotation files')
        )

    if test_argv:
        args = parser.parse_args(test_argv)
    else:
        args = parser.parse_args()
        check_args(parser, args)

    set_global_vars(args)

    return args

def set_global_vars(args):

    global cores

    if args.command == 'make_emapper_input':
        cores = args.cores

def check_args(parser, args):

    if args.command == 'make_emapper_input':

        if args.postnovo_seqs == None:
            parser.error('query fasta input must be provided')
        if args.postnovo_seqs != None:
            if not os.path.exists(args.postnovo_seqs):
                parser.error(args.postnovo_seqs + ' does not exist')

        if args.redo_without_search == False:
            if args.diamond == None:
                parser.error('diamond filepath needed')
            if not os.path.exists(args.diamond):
                parser.error(args.diamond + ' does not exist')
            if args.diamond_db == None:
                parser.error('DIAMOND database filepath must be provided')
            if not os.path.exists(args.diamond_db):
                parser.error(args.diamond_db + ' does not exist')
            if args.taxonmap == None:
                parser.error('taxonmap filepath must be provided')
            if not os.path.exists(args.taxonmap):
                parser.error(args.taxonmap + ' does not exist')

            if args.blastp == None:
                parser.error('blastp filepath needed')
            if not os.path.exists(args.blastp):
                parser.error(args.blastp + ' does not exist')
            if args.blast_dbs == None:
                parser.error('BLAST+ database filepath(s) must be provided')
            for db_fp in args.blast_dbs:
                db_dir = os.path.dirname(db_fp)
                db_name = os.path.basename(db_fp)
                if not os.path.exists(db_dir):
                    parser.error(db_dir + ' does not exist')
                if not os.path.exists(os.path.join(db_dir, db_name + '.pal')):
                    parser.error(db_name + ' does not exist in ' + db_dir)

        if args.max_seqs_per_blast_instance < 1:
            parser.error(str(args.max_seqs_per_blast_instance) + ' must be a positive number')

        if args.cores < 1 or args.cores > cpu_count():
            parser.error(str(cpu_count()) + ' cores are available')

    # analyze_eggnog_output subcommand
    elif args.command == 'analyze_emapper_output':
        if not os.path.exists(args.iodir):
            parser.error(args.iodir + ' does not exist')
        if (
            not os.path.exists(os.path.join(args.iodir, 'sampled_df.csv')) or 
            not os.path.exists(os.path.join(args.iodir, 'low_prob_profile_df.csv')) or
            not os.path.exists(os.path.join(args.iodir, 'high_prob_taxa_assign_df.csv')) or 
            not os.path.exists(os.path.join(args.iodir, 'eggnog_fasta_path_list.pkl'))
            ):
            parser.error(
                'the following files must be present in ' + 
                args.iodir + 
                ': sampled_df.csv, '
                'low_prob_profile_df.csv, '
                'high_prob_taxa_assign_df.csv, '
                'eggnog_fasta_path_list.pkl'
                )

def split_blast_fasta(fasta_list, max_seqs_per_process):
    '''
    To run multiple BLAST+ processes at once,
    split the fasta file into a number equal to the cores available.
    Returns a list of the new fasta filepaths.
    '''

    split_fasta_fp_list = []
    fasta_basename = os.path.splitext(
        os.path.basename(fasta_input_fp)
        )[0]
    parent_fasta_size = len(fasta_list) / 2
    if parent_fasta_size % int(parent_fasta_size) > 0:
        raise ValueError('The fasta input must have an even number of lines.')
    child_fasta_size = int(parent_fasta_size / cores)
    remainder = parent_fasta_size % cores

    if child_fasta_size + remainder < max_seqs_per_process:
        for core in range(cores):
            child_fasta_list = fasta_list[
                core * child_fasta_size * 2: (core + 1) * child_fasta_size * 2
                ]
            child_fasta_fp = os.path.join(
                out_dir, fasta_basename + '_' + str(core + 1) + '.blastp-short.faa'
                )
            with open(child_fasta_fp, 'w') as child_fasta_file:
                for line in child_fasta_list:
                    child_fasta_file.write(line)
            split_fasta_fp_list.append(child_fasta_fp)
        with open(child_fasta_fp, 'a') as child_fasta_file:
            child_fasta_list = fasta_list[cores * child_fasta_size * 2:]
            for line in child_fasta_list:
                child_fasta_file.write(line)
    else:
        fasta_line = 0
        child_fasta_count = 1
        while fasta_line < len(fasta_list):
            child_fasta_list = fasta_list[
                fasta_line: fasta_line + max_seqs_per_process * 2
                ]
            child_fasta_filename = os.path.join(
                out_dir, fasta_basename + '_' + str(child_fasta_count) + '.blastp-short.faa'
                )
            with open(child_fasta_filename, 'w') as child_fasta_file:
                for line in child_fasta_list:
                    child_fasta_file.write(line)
            split_fasta_fp_list.append(child_fasta_filename)
            fasta_line += max_seqs_per_process * 2
            child_fasta_count += 1

    return split_fasta_fp_list

def run_diamond(fasta_fp, diamond_fp, db_fp, taxonmap, cores):
    
    subprocess.call([
        diamond_fp,
        'blastp',
        '--threads', str(cores),
        '--db', db_fp,
        '--query', fasta_fp,
        '--taxonmap', taxonmap,
        '--more-sensitive',
        '--out', os.path.splitext(fasta_fp)[0] + '.out',
        '--outfmt', '6', 'qseqid', 'sseqid', 'evalue', 'bitscore', 'staxids', 'salltitles',
        '--evalue', '0.1',
        '--max-hsps', '1',
        '--unal', '1',
        '--block-size', '20',
        '--index-chunks', '1'
        ])

def run_blast(db_fp_list, fasta_fp_list, blastp_fp):

    # Record where the blast db's will be loaded into memory
    original_db_fp_dict = OrderedDict()
    loaded_db_fp_dict = OrderedDict()
    for db_fp in db_fp_list:
        db_name = os.path.basename(db_fp)
        original_db_fp_dict[db_name] = db_fp
        loaded_db_fp_dict[db_name] = os.path.join(
            os.path.join('/dev/shm', db_name), db_name
            )

    # Set the number of heavyweight BLAST+ processes (more memory-intensive)
    # and number of threads (less memory-intensive) for each process
    max_processes_allowed = min([cores, 16])
    num_threads = cores // max_processes_allowed

    # Make a list of all the BLAST+ commands to be executed
    commands = []
    out_fp_list = []
    for db_name, loaded_db_fp in loaded_db_fp_dict.items():
        for fasta_fp in fasta_fp_list:
            # Incorporate the name of the db into the BLAST+ output file
            out_fp = os.path.splitext(fasta_fp)[0] + '_' + db_name + '.out'
            commands.append([
                blastp_fp,
                '-task', 'blastp-short',
                '-db', loaded_db_fp,
                '-query', fasta_fp,
                '-out', out_fp,
                '-evalue', '1000000',
                '-max_target_seqs', '500',
                '-max_hsps', '1',
                '-comp_based_stats', '0',
                '-num_threads', str(num_threads),
                '-outfmt', '6 qseqid sacc evalue bitscore salltitles'
                ])
            out_fp_list.append(out_fp)

    # Run the BLAST+ commands, loading and unloading db's from mem as needed
    processes = []
    running_commands = []
    dbs_in_mem = []
    instances_started = 0
    while len(processes) > 0 or instances_started == 0:
        while len(processes) < max_processes_allowed and instances_started < len(fasta_fp_list) * len(db_fp_list):
            command = commands[instances_started]
            running_commands.append(command)
            # Unload any BLAST db's not in use
            dbs_in_use = []
            for running_command in running_commands:
                dbs_in_use.append(running_command[running_command.index('-db') + 1])
            dbs_in_use = list(set(dbs_in_use))
            for db_in_mem in dbs_in_mem:
                if db_in_mem not in dbs_in_use:
                    subprocess.call(['rm', '-r', os.path.dirname(db_in_mem)])
            # Load the BLAST db into memory if not already loaded
            loaded_db_fp = command[command.index('-db') + 1]
            if loaded_db_fp not in dbs_in_mem:
                db_name = os.path.basename(loaded_db_fp)
                subprocess.call([
                    'rm', '-rf',
                    os.path.dirname(loaded_db_fp)
                    ])
                subprocess.call([
                    'mkdir',
                    os.path.dirname(loaded_db_fp)
                    ])
                db_file_list = glob.glob(os.path.join(os.path.dirname(original_db_fp_dict[db_name]), '*'))
                subprocess.call(
                    ['cp'] + 
                    db_file_list + 
                    [os.path.dirname(loaded_db_fp)]
                    )
                dbs_in_mem.append(loaded_db_fp)

            process = subprocess.Popen(command)
            processes.append(process)
            instances_started += 1
        for i, process in enumerate(processes):
            if process.poll() is not None:
                processes.remove(process)
        time.sleep(30)

    # Unload any residual BLAST db's
    for db_in_mem in dbs_in_mem:
        subprocess.call(['rm', '-r', os.path.dirname(db_in_mem)])

    return out_fp_list

#def run_blast(fasta_fp_list, blastp_fp, db_fp):
    
#    processes = []
#    instances_started = 0
#    while len(processes) > 0 or instances_started == 0:
#        while len(processes) < cores and instances_started < len(fasta_fp_list):
#            fasta_fp = fasta_fp_list[instances_started]
#            out_fp = os.path.join(
#                os.path.dirname(fasta_fp),
#                os.path.splitext(os.path.basename(fasta_fp))[0] + '.out'
#                )
#            process = subprocess.Popen([
#                blastp_fp,
#                '-task', 'blastp-short',
#                '-db', db_fp,
#                '-query', fasta_fp,
#                '-out', out_fp,
#                '-evalue', '1000000',
#                '-max_target_seqs', '500',
#                '-max_hsps', '1',
#                '-comp_based_stats', '0',
#                '-outfmt', '6 qseqid sgi sacc evalue bitscore staxids salltitles',
#                ])
#            processes.append(process)
#            instances_started += 1
#            #print('number of instances started: ' + str(instances_started), flush=True)
#        for i, process in enumerate(processes):
#            #print('process poll for ' + str(i) + ': ' + str(process.poll()), flush=True)
#            if process.poll() is not None:
#                processes.remove(process)
#        # REMOVE
#        #pl = subprocess.Popen(['ps'], stdout=subprocess.PIPE).communicate()[0]
#        #print(pl, flush=True)
#        time.sleep(30)

    ## Script name modified
    ##blast_batch_fp = resource_filename('postnovo', 'bashscripts/blast_batch.sh')
    #blast_batch_fp = resource_filename('postnovo', 'bashscripts/blast_batch_simple.sh')
    #with open(blast_batch_fp) as blast_batch_template_file:
    #    blast_batch_template = blast_batch_template_file.read()
    #temp_blast_batch_script = blast_batch_template
    #temp_blast_batch_script = temp_blast_batch_script.replace('FASTA_FILES=', 'FASTA_FILES=({})'.format(' '.join(fasta_fp_list)))
    #temp_blast_batch_script = temp_blast_batch_script.replace('MAX_PROCESSES=', 'MAX_PROCESSES={}'.format(cores - 1))
    #temp_blast_batch_script = temp_blast_batch_script.replace('BLASTP_PATH=', 'BLASTP_PATH={}'.format(blastp_fp))
    #temp_blast_batch_script = temp_blast_batch_script.replace('DB_DIR=', 'DB_DIR={}'.format(db_fp))
    ## Script name modified
    ##temp_blast_batch_fp = os.path.join(os.path.dirname(blast_batch_fp), 'blast_batch~.sh')
    #temp_blast_batch_fp = os.path.join(os.path.dirname(blast_batch_fp), 'blast_batch_simple~.sh')
    #with open(temp_blast_batch_fp, 'w') as temp_blast_batch_file:
    #    temp_blast_batch_file.write(temp_blast_batch_script)
    #os.chmod(temp_blast_batch_fp, 0o777)
    #subprocess.call([temp_blast_batch_fp])

def get_linnean_hierarchy(high_prob_out_table, lower_prob_out_table, filtered_out_table):
    '''
    Retrieve Linnean hierarchy from NCBI taxonomy db given taxid
    '''

    taxids = filtered_out_table['taxid'].unique().tolist()
    lineage_dict = OrderedDict().fromkeys(taxids)
    one_percent_number_taxid = len(taxids) / 100 / cores
    rank_dict = OrderedDict().fromkeys(search_ranks)
    search_ranks_set = set(search_ranks)

    # Query the NCBI database to retrieve the Linnean lineage of each taxid
    ## Single process
    #lineage_lists = []
    #print_percent_progress_fn = partial(
    #    utils.print_percent_progress_singlethreaded,
    #    procedure_str='Taxonomic hierarchy recovery progress: ',
    #    one_percent_total_count=one_percent_number_taxid * cores
    #    )
    #single_var_get_lineage = partial(
    #    get_lineage,
    #    rank_dict=rank_dict, 
    #    search_ranks_set=search_ranks_set, 
    #    print_percent_progress_fn=print_percent_progress_fn
    #    )
    #for taxid in taxids:
    #    lineage_lists.append(single_var_get_lineage(taxid))

    # Multiprocess
    print_percent_progress_fn = partial(
        utils.print_percent_progress_multithreaded,
        procedure_str='Taxonomic hierarchy recovery progress: ',
        one_percent_total_count=one_percent_number_taxid,
        cores=cores
        )
    single_var_get_lineage = partial(
        get_lineage,
        rank_dict=rank_dict, 
        search_ranks_set=search_ranks_set, 
        print_percent_progress_fn=print_percent_progress_fn
        )
    multiprocessing_pool = Pool(cores)
    lineage_lists = multiprocessing_pool.map(
        single_var_get_lineage, taxids
        )
    multiprocessing_pool.close()
    multiprocessing_pool.join()

    for i, taxid in enumerate(taxids):
        lineage_dict[taxid] = lineage_lists[i]

    # Append the lineage info to each row (alignment) of the full filtered output table
    lineage_appendix_list = []
    for taxid in filtered_out_table['taxid'].tolist():
        lineage_appendix_list.append(lineage_dict[taxid])
    lineage_appendix_table = pd.DataFrame(lineage_appendix_list, columns=search_ranks)
    augmented_blast_table = pd.concat([filtered_out_table, lineage_table], axis=1)

    # Use the row ID's from the full filtered output table
    # to merge lineage info into the high and lower prob output tables
    merge_table = augmented_blast_table[['seq_number', 'hit_number'] + search_ranks]
    high_prob_out_table = high_prob_out_table.merge(merge_table, on=['seq_number', 'hit_number'])
    lower_prob_out_table = lower_prob_out_table.merge(merge_table, on=['seq_number', 'hit_number'])

    return high_prob_out_table, lower_prob_out_table, filtered_out_table

def get_lineage(taxid, rank_dict, search_ranks_set, print_percent_progress_fn):
    '''
    Given a taxid, retrieve the taxon and lineage from NCBI Taxonomy db
    '''

    print_percent_progress_fn()

    # Occasionally, Entrez will return corrupted information
    bad_lineage = True
    q_count = 0
    # I have found that the returned information is occasionally corrupted
    # Therefore, check for a bogus lineage,
    # and repeat the query up to 5 times because of this
    while bad_lineage and q_count < 5:
        q_count += 1
        # Keep trying to connect to NCBI
        while True:
            try:
                taxon_info = Entrez.read(Entrez.efetch(db='Taxonomy', id=taxid))
                break
            except:
                print('Waiting for taxid ' + str(taxid), flush=True)
                time.sleep(2)
        # Taxids can correspond to species, genera, orders, etc...
        taxon = taxon_info[0]['ScientificName']
        taxon_rank = taxon_info[0]['Rank']
        lineage_info = taxon_info[0]['LineageEx']

        taxon_ranks_set = set()
        # Record the rank corresponding to the taxid
        if taxon_rank in search_ranks_set:
            rank_dict[taxon_rank] = taxon
            taxon_ranks_set.add(taxon_rank)
        # Record the "coarser" ranks
        for entry in lineage_info:
            rank = entry['Rank']
            if rank in search_ranks_set:
                rank_dict[rank] = entry['ScientificName']
                taxon_ranks_set.add(rank)
        # Record an empty string for each "finer" rank not within the taxid lineage
        for rank in search_ranks_set.difference(taxon_ranks_set):
            rank_dict[rank] = ''

        lineage_list = [level for level in rank_dict.values()]
        # Check that the coarsest rank is a valid superkingdom (rather than a phylum, say)
        if lineage_list[-1] not in superkingdoms:
            print('Querying again due to bad lineage: ', taxid, taxon_info)
        else:
            bad_lineage = False

    return lineage_list

def assign_taxonomy(align_table):

    # Make a separate table for each query seq in the table of alignment results
    seq_align_tables = [seq_align_table for _, seq_align_table in align_table.groupby('seq_number')]
    # Rows of parsimonious taxonomic assignments, each corresponding to a query seq
    taxa_assignments = []
    for seq_align_table in seq_align_tables:
        # Only consider a subset of hits representing a unique set of taxids
        representative_hits = seq_align_table.drop_duplicates(subset=['taxid'])
        # Loop through each taxonomic rank that is considered
        for rank_index, rank in enumerate(search_ranks):
            # At the given rank, find the most common Linnean name
            most_common_name = Counter(representative_hits[rank]).most_common(1)[0]
            # Proceed if the lineage was assigned at the rank
            if most_common_name[0] != '' and pd.notnull(most_common_name[0]):
                # Proceed if the Linnean name applies to at least a minimum proportion of taxids
                if most_common_name[1] >= taxon_assignment_threshold * len(representative_hits):
                    # Take as the best hit one with the Linnean name and the lowest e-value (first in the table)
                    representative_hit = representative_hits.ix[
                        representative_hits[
                            representative_hits[rank] == most_common_name[0]
                            ][rank].first_valid_index()
                        ]
                    taxa_assignments.append(
                        representative_hit[seq_info_hdr].tolist() + 
                        rank_index * ['N/A'] + 
                        representative_hit[rank:].tolist()
                        )
                    break
        # Without a break in the loop due to discovery of a "finest-grained" taxonomic name in common among hits,
        # record that no common taxonomy could be found for the seq (no name at any rank)
        else:
            representative_hit = representative_hits.iloc[0]
            taxa_assignments.append(
                representative_hit[seq_info_hdr].tolist() + 
                len(search_ranks) * ['N/A']
                )

    # Make a DataFrame from the lists of taxonomic information for each seq
    taxa_table = pd.DataFrame(taxa_assignments, columns=seq_info_hdr + search_ranks)
    # Determine the number of assignments to each taxonomic name at each rank
    # This is less a table than a set of ordered cols
    # with the names 'species name', 'species count', 'genus name', 'genus count', etc.
    taxa_count_table = pd.DataFrame()
    for rank in search_ranks:
        taxa_counts = Counter(taxa_table[rank])
        taxa_count_table = pd.concat(
            [taxa_count_table,
             pd.Series([taxon for taxon in taxa_counts.keys()], name = rank + ' taxa'),
             pd.Series([count for count in taxa_counts.values()], name = rank + ' count')],
             axis = 1
             )

    return taxa_table, taxa_count_table

def filter_lower_prob_taxa(high_prob_taxa_table, high_prob_taxa_count_table, high_prob_out_table, lower_prob_out_table):
    '''
    Select lower-probability results that conform with the high-probability taxonomic profile
    '''

    # The "taxonomic profile" of the high-prob taxa
    # is the set of taxonomic assignments at the family, genus or species level
    # represented by a minimum proportion of the high-prob taxa
    profile_proportion_cutoff = 0.01

    taxa_profile_dict = OrderedDict()
    for rank in profile_ranks:
        high_prob_seq_count = high_prob_taxa_count_table[rank + ' count'].sum()
        # The cols of high-prob taxonomic info are uneven for each rank,
        # so check for null values at the tail of the cols
        high_prob_name_list = [
            name for name in high_prob_taxa_count_table[rank + ' name'].tolist()
            if pd.notnull(name)
            ]
        high_prob_count_list = [
            count for count in high_prob_taxa_count_table[rank + ' count'].tolist()
            if pd.notnull(count)
            ]
        count_dict = OrderedDict([
            (name_list[i], count_list[i]) for i in range(len(name_list))
            ])
        taxa_profile_dict[rank] = [
            name for name in count_dict
            if count_dict[name] >= profile_proportion_cutoff * high_prob_seq_count
            ]

    # Record if each lower-prob seq falls in the high-prob taxonomic profile
    hit_in_profile_list = [False for _ in range(len(lower_prob_out_table))]
    for rank in profile_ranks:
        profile = taxa_profile_dict[rank]
        lower_prob_names = lower_prob_out_table[rank].tolist()
        for i, lower_prob_name in enumerate(lower_prob_names):
            if not hit_in_profile_list[i]:
                if lower_prob_name in profile:
                    hit_in_profile_list[i] = True
    lower_prob_out_table['in_taxa_profile']

    lower_prob_out_table = lower_prob_out_table[lower_prob_out_table['in_taxa_profile'] is True]
    lower_prob_out_table.drop('in_taxa_profile', inplace=True)
    filtered_out_table = pd.concat([high_prob_out_table, lower_prob_out_table], ignore_index=True)

    return filtered_out_table, lower_prob_out_table

def sample_hits(seq_retained_hits, sample_size=10):
    ''' Sample hits retained within the taxonomic profile for a query sequence '''
    
    # Only consider hits within 2 bits of the top hit
    top_hits = seq_retained_hits[
        seq_retained_hits['bitscore'] >= (seq_retained_hits['bitscore'].max() - 2)
        ]

    number_top_hits = len(top_hits)
    if number_top_hits <= 10:
        return top_hits
    else:
        hit_indices = list(range(number_top_hits))
        rounded_quotient, remainder = divmod(len(hit_indices), sample_size)
        sample_indices = [hit_indices[i * rounded_quotient + min(i, remainder)] for i in range(number_top_hits)]
        return top_hits.iloc[sample_indices]

def get_full_subject_seqs(accessions):

    one_percent_number_seqs = len(accession_list) / 100 / cores

    # Multiprocess
    multiprocessing_pool = Pool(cores)
    print_percent_progress_fn = partial(
        utils.print_percent_progress_multithreaded,
        procedure_str = 'Full subject sequence recovery progress: ',
        one_percent_total_count = one_percent_number_seqs,
        cores = cores
        )
    single_var_get_full_subject_seq = partial(
        get_full_subject_seq,
        print_percent_progress_fn = print_percent_progress_fn
        )
    full_subject_seqs = multiprocessing_pool.map(single_var_get_full_subject_seq, accessions)
    multiprocessing_pool.close()
    multiprocessing_pool.join()

    return full_subject_seqs

def get_full_subject_seq(accession, print_percent_progress_fn):

    print_percent_progress_fn()

    while True:
        try:
            full_seq = Entrez.read(
                Entrez.efetch(db='Protein', id=accession, retmode='xml')
                )[0]['GBSeq_sequence']
            break
        except:
            print('Waiting for ' + accession, flush=True)
            time.sleep(2)

    return full_seq

def write_eggnog_fasta_files(sampled_out_table, out_dir, fasta_filename_prefix):
    
    sampled_out_table['seq_number'] = sampled_out_table['seq_number'].apply(str)
    sampled_out_table['hit_number'] = sampled_out_table['hit_number'].apply(str)
    eggnog_fasta_paths = []
    # Make the fasta files for each superkingdom
    for superkingdom in superkingdoms:
        # Consider the retained hits to the superkingdom
        superkingdom_out_table = sampled_out_table[sampled_out_table['superkingdom'] == superkingdom]
        # Only write to file if there is data for the superkingdom
        if len(superkingdom_out_table) > 0:
            seq_numbers = superkingdom_out_table['seq_number'].tolist()
            hit_numbers = superkingdom_out_table['hit_number'].tolist()
            full_subject_seqs = superkingdom_out_table['full_subject_seq'].tolist()
            # The seq_number and hit_number identify the specific alignment
            fasta_headers = [
                '>' + '(seq_number)' + seq_numbers[i] + '(hit_number)' + full_subject_seqs[i]
                for i in range(len(seq_numbers))
                ]
            fasta_seq_dict = OrderedDict([
                (fasta_headers[i], full_subject_seqs[i]) for i in range(len(fasta_headers))
                ])

            # Online emapper with hmmr allows up to 5,000 seqs per file
            fasta_seq_count_limit = 5000
            generic_fasta_filename = fasta_filename_prefix + '_emapper.faa'
            # Keep track of the index of the next seq to go into a fasta file
            hit_index = 0
            # Loop through each batch of hits to go into a file
            for i in range(len(fasta_headers) // fasta_seq_count_limit + 1):
                path = os.path.join(
                    out_dir, os.path.splitext(generic_fasta_filename)[0] + '.' + superkingdom.lower() + '_' + str(i) + '.faa'
                    )
                eggnog_fasta_paths.append(path)
                # Open a new fasta file
                with open(path, 'w') as handle:
                    # Loop through each hit to be included in the fasta file
                    for j, header in enumerate(fasta_headers[hit_index:]):
                        handle.write(header + '\n')
                        handle.write(fasta_seq_dict[header] + '\n')
                        # Stop writing to the file when the 5,000 seq limit is reached
                        if j + 1 == fasta_seq_count_limit:
                            # Calculate the index of the first seq in the next file
                            hit_index = (j + 1) * (i + 1)
                            break

    return eggnog_fasta_paths

def parse_eggnog_mapper_output(eggnog_out_fp_list):

    eggnog_df = pd.DataFrame(columns=eggnog_output_headers)
    for out_fp in eggnog_out_fp_list:
        if os.path.exists(out_fp):
            if os.stat(out_fp).st_size > 0:
                eggnog_output_df = pd.read_csv(out_fp, sep='\t', header=None, names=eggnog_output_headers, comment='#')
                eggnog_df = pd.concat([eggnog_df, eggnog_output_df], ignore_index=True)

    # Split header into two cols for scan lists and hits
    query_list = eggnog_df['query'].tolist()
    query_list = [query.split('(' + id_type + ')')[1] for query in query_list]
    temp_list_of_lists = [query.split('(hit)') for query in query_list]
    id_list = [temp_list[0] for temp_list in temp_list_of_lists]
    hit_list = [temp_list[1] for temp_list in temp_list_of_lists]
    eggnog_df.drop('query', axis=1, inplace=True)
    eggnog_df[id_type] = id_list
    eggnog_df['hit'] = hit_list

    return eggnog_df

def find_coherent_hits(eggnog_df, sampled_df):

    # Need to account for queries that do not have an eggnog annotation
    # Need a table of all id + hit
    # Groupby id, make sorted list of hit
    # Sorted list of hits for id must match that from eggnog annotations to call annotations conserved

    conserv_func_df_list = []
    eggnog_df_groups = eggnog_df.groupby(id_type)
    sampled_df_groups = sampled_df.groupby(id_type)
    for id in list(set(eggnog_df[id_type])):
        eggnog_df_group = eggnog_df_groups.get_group(id)
        sampled_df_group = sampled_df_groups.get_group(id)
        # If all annotated seqs have the same description string
        if (eggnog_df_group['eggnog hmm desc'] == eggnog_df_group['eggnog hmm desc'].iloc[0]).all():
            # If all the queries (sampled blast hits) are present in the annotation file
            if len(eggnog_df_group) == len(sampled_df_group):
                conserv_func_df_list.append(eggnog_df_group)
    conserv_func_df = pd.concat(conserv_func_df_list, ignore_index=True)
    return conserv_func_df

def condense_hits(conserv_func_df, low_prob_profile_df, high_prob_taxa_assign_df):

    # Condense rows of each group
    conserv_func_df = conserv_func_df.fillna('')
    id_set_list = list(set(conserv_func_df['scan_list'].tolist()))
    parsed_conserv_func_df_cols_dict = OrderedDict(
        [(header, []) for header in parsed_conserv_func_df_headers_list])
    parsed_conserv_func_df = pd.DataFrame()
    parsed_conserv_func_df[id_type] = id_set_list
    conserv_func_df_groups = conserv_func_df.groupby(id_type)
    # Loop through each group
    for id in id_set_list:
        conserv_func_df_group = conserv_func_df_groups.get_group(id)
        # Go through each col in group
        # seed ortholog: set intersection
        parsed_conserv_func_df_cols_dict['seed ortholog'].append(
            ','.join(list(set(conserv_func_df_group['seed ortholog']))))
        # evalue: ignore
        # score: ignore
        # predicted name: set intersection
        parsed_conserv_func_df_cols_dict['predicted name'].append(
            ','.join(list(set(conserv_func_df_group['predicted name']))))
        # go terms: set intersection
        list_of_lists = conserv_func_df_group['go terms'].apply(lambda x: x.split(',')).tolist()
        l = [i for sublist in list_of_lists for i in sublist]
        parsed_conserv_func_df_cols_dict['go terms'].append(','.join(list(set(l))))
        # kegg pathways: set intersection
        list_of_lists = conserv_func_df_group['kegg pathways'].apply(lambda x: x.split(',')).tolist()
        l = [i for sublist in list_of_lists for i in sublist]
        parsed_conserv_func_df_cols_dict['kegg pathways'].append(
            ','.join(list(set(l))))
        # tax scope: set intersection
        parsed_conserv_func_df_cols_dict['tax scope'].append(
            ','.join(list(set(conserv_func_df_group['tax scope']))))
        # eggnog ogs: set intersection
        list_of_lists = conserv_func_df_group['eggnog ogs'].apply(lambda x: x.split(',')).tolist()
        l = [i for sublist in list_of_lists for i in sublist]
        parsed_conserv_func_df_cols_dict['eggnog ogs'].append(
            ','.join(list(set(l))))
        # best og: set intersection of first substring before |
        parsed_conserv_func_df_cols_dict['best og'].append(
            ','.join(list(set(
                conserv_func_df_group['best og'].apply(lambda x: x.split('|')[0])))))
        # cog cat: set intersection
        parsed_conserv_func_df_cols_dict['cog cat'].append(
            ','.join(list(set(conserv_func_df_group['cog cat']))))
        # eggnog hmm desc: first row (all rows the same)
        parsed_conserv_func_df_cols_dict['eggnog hmm desc'].append(
            conserv_func_df_group['eggnog hmm desc'].iloc[0])
        # scan_list: already placed in parsed_conserv_func_df
        # hit: ignore
    for header, col in parsed_conserv_func_df_cols_dict.items():
        parsed_conserv_func_df[header] = col

    low_prob_profile_df.set_index(id_type, inplace=True)
    id_set_list = set(id_set_list).intersection(set(low_prob_profile_df.index.tolist()))
    low_prob_profile_df = low_prob_profile_df.loc[id_set_list]
    low_prob_profile_df.reset_index(inplace=True)
    low_prob_profile_df = low_prob_profile_df.drop('is_in_profile', axis=1)
    low_prob_profile_taxa_assign_df, low_prob_profile_taxa_count_df = \
        find_parsimonious_taxonomy(low_prob_profile_df)

    if 'hit' in high_prob_taxa_assign_df.columns:
        high_prob_taxa_assign_df.drop('hit', axis=1, inplace=True)

    profile_taxa_assign_df = pd.concat([high_prob_taxa_assign_df, low_prob_profile_taxa_assign_df], ignore_index=True)

    reported_df = parsed_conserv_func_df.merge(profile_taxa_assign_df, how='inner', on=id_type)
    reported_df.fillna('', inplace=True)
    reported_df.replace('N/A', '', inplace=True)
    # Put double quotes around scan_list values
    return reported_df

if __name__ == '__main__':
    main()