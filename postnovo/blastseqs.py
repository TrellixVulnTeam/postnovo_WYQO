import argparse
import copy
import glob
import numpy as np
import os
import pandas as pd
import pkg_resources
import pickle as pkl
import re
import subprocess
import sys
import time

from Bio import Entrez
from collections import OrderedDict, Counter
from functools import partial
from multiprocessing import Pool, cpu_count
from pkg_resources import resource_filename

if 'postnovo' in sys.modules:
    import postnovo.config as config
    import postnovo.utils as utils
else:
    import config
    import utils

local_eggnog = False
fasta_input_fp = ''
out_dir = ''

min_seq_len_for_diamond = 31

Entrez.email = 'samuelmiller@uchicago.edu'
cores = 1

blast_out_hdr = [
    'qseqid',
    'accession',
    'evalue',
    'bitscore', 
    'stitle'
    ]

max_float = np.finfo('f').max
max_int = np.finfo('d').max

superkingdoms = [
    'Archaea', 
    'Bacteria',
    'Eukaryota',
    'Viruses'
    ]
search_ranks = [
    'species', 
    'genus', 
    'family', 
    'order', 
    'class', 
    'phylum', 
    'superkingdom'
    ]
taxon_assignment_threshold = 0.9

emapper_abbr_dict = {
    'archaea': 'arch',
    'bacteria': 'bact', 
    'eukaryota': 'euk', 
    'viruses': 'viruses'
    }

eggnog_output_headers = [
    'query', 
    'seed ortholog', 
    'evalue', 
    'score', 
    'predicted name',
    'go terms', 
    'kegg pathways', 
    'tax scope', 
    'eggnog ogs', 
    'best og',
    'cog cat', 
    'eggnog hmm desc'
    ]

parsed_conserv_func_df_headers_list = [
    'seed ortholog', 
    'predicted name', 
    'go terms', 
    'kegg pathways',
    'tax scope', 
    'eggnog ogs', 
    'best og', 
    'cog cat', 
    'eggnog hmm desc'
    ]

taxa_profile_evalue = 0.1

def main(test_argv=None):
    global local_eggnog, out_dir

    test_argv = []
    if 'test_argv' in locals():
        args = parse_args(test_argv)
    else:
        args = parse_args()

    if args.command == 'make_emapper_input':
        if args.local_eggnog:
            local_eggnog = True
        make_query_files(args)
    elif args.command == 'analyze_eggnog_output':
        out_dir = args.iodir

        # Load files generated by make_emapper_input
        sampled_df = pd.read_csv(
            os.path.join(out_dir, 'sampled_df.csv')
            )
        low_prob_profile_df = pd.read_csv(
            os.path.join(out_dir, 'low_prob_profile_df.csv')
            )
        high_prob_taxa_assign_df = pd.read_csv(
            os.path.join(out_dir, 'high_prob_taxa_assign_df.csv')
            )
        with open(os.path.join(out_dir, 'eggnog_fasta_path_list.pkl'), 'rb') as f:
            eggnog_fasta_path_list = pkl.load(f)

        analyze_eggnog_output(
            args, sampled_df, low_prob_profile_df, high_prob_taxa_assign_df, eggnog_fasta_path_list
            )

def make_query_files(args):
    
    global fasta_input_fp, out_dir
    fasta_list = open(args.postnovo_seqs, 'r').readlines()
    fasta_input_fp = args.postnovo_seqs

    out_dir = os.path.dirname(fasta_input_fp)

    # If BLAST+ and DIAMOND have already been run,
    if args.redo_without_search:

        out_dir_files = [f for f in os.listdir(out_dir) if os.path.isfile(os.path.join(out_dir, f))]
        diamond_out_fp = ''
        split_blast_out_fp_list = []
        fasta_basename = os.path.splitext(os.path.basename(fasta_input_fp))[0]
        for f in out_dir_files:
            if fasta_basename + '.diamond.out' in f:
                if diamond_out_fp != '':
                    raise Exception('DIAMOND output of the name ' + os.path.splitext(fasta_input_fp)[0] + '.diamond.out already exists')
            elif fasta_basename in f and 'blastp-short.out' in f and os.path.splitext(f)[1] == '.out':
                split_blast_out_fp_list.append(os.path.join(out_dir, f))

    else:

        # Check that the input fasta file looks like a fasta file
        parent_fasta_size = len(fasta_list) / 2
        if parent_fasta_size % int(parent_fasta_size) > 0:
            raise ValueError('The fasta input must have an even number of lines.')

        # Separate fasta entries into longer seqs for DIAMOND and shorter seqs for BLAST
        diamond_fasta_list = []
        blast_fasta_list = []
        entry = []
        for line in fasta_list:
            entry.append(line)
            if line[0] != '>':
                if len(line.rstrip()) >= min_seq_len_for_diamond:
                    diamond_fasta_list += entry
                else:
                    blast_fasta_list += entry
                entry = []

        # Write fasta file for DIAMOND
        diamond_fasta_fp = os.path.splitext(fasta_input_fp)[0] + '.diamond.faa'
        with open(diamond_fasta_fp, 'w') as handle:
            for line in diamond_fasta_list:
                handle.write(line)

        # To run BLAST+ on multiple CPUs, split the fasta input into multiple files
        split_blast_fasta_fp_list = split_blast_fasta(blast_fasta_list, args.max_seqs_per_blast_instance)

        #run_diamond(diamond_fasta_fp, args.diamond, args.diamond_db, args.taxonmap, args.cores)
        split_blast_out_fp_list = run_blast(args.blast_dbs, split_blast_fasta_fp_list, args.blastp)        

        sys.exit()

    for blast_db_fp in args.blast_dbs:
        db_name = os.path.basename(blast_db_fp)

        # Merge all of the split BLAST tables into a single table
        merged_blast_table = pd.DataFrame(columns=blast_out_hdr)
        for out_fp in split_blast_out_fp_list:
            partial_blast_table = pd.read_csv(out_fp, sep='\t', header=None, names=blast_out_hdr)
            merged_blast_table = pd.concat(
                [merged_blast_table, partial_blast_table], ignore_index=True
                )

        # Retrieve taxids from the accession2taxid file
        subject_accession_list = merged_blast_table['accession'].tolist()
        subject_accession_dict = OrderedDict().fromkeys(subject_accession_list)
        taxonmap_df = pd.read_csv(args.taxonmap, sep='\t', header=0)[['accession', 'taxid']]
        taxonmap_accession_list = taxonmap_df['accession']
        taxonmap_taxid_list = taxonmap_df['taxid']
        for i, taxonmap_accession in taxonmap_accession_list:
            if taxonmap_accession in subject_accession_list:
                subject_accession_dict[taxonmap_accession] = taxonmap_taxid_list[i]
        subject_taxid_list = []
        for subject_accession in subject_accession_list:
            subject_taxid_list.append(subject_accession_dict[subject_accession])
        merged_blast_table['taxid'] = subject_taxid_list

        if args.intermediate_files:
            merged_blast_table.to_csv(
                os.path.join(out_dir, db_name + '_merged_df.csv'), index=False
                )
        parsed_blast_table = parse_blast_table(fasta_input_fp, merged_blast_table)
        if args.intermediate_files:
            parsed_blast_table.to_csv(
                os.path.join(out_dir, db_name + '_parsed_blast_table.csv'), index=False
                )
        parsed_blast_table_list.append(parsed_blast_table)

    #merged_blast_table = pd.DataFrame(columns=blast_hdr)
    #for out_fp in blast_out_fp_list:
    #    part_blast_table = pd.read_csv(out_fp, sep='\t', header=None, names=blast_hdr)
    #    merged_blast_table = pd.concat(
    #        [merged_blast_table, part_blast_table], ignore_index=True
    #        )
    #if args.intermediate_files:
    #    merged_blast_table.to_csv(
    #        os.path.join(out_dir, db_name + '_merged_df.csv'), index=False
    #        )
    #parsed_blast_table = parse_blast_table(fasta_input_fp, merged_blast_table)
    #if args.intermediate_files:
    #    parsed_blast_table.to_csv(
    #        os.path.join(out_dir, db_name + '_parsed_blast_table.csv'), index=False
    #        )
    #parsed_blast_table_list.append(parsed_blast_table)

    #if len(parsed_blast_table_list) == 1:
    #    parsed_blast_table = parsed_blast_table_list[0]
    #else:
    #    # multiple blast db's were queried
    #    parsed_blast_table = optimize_blast_table(parsed_blast_table_list)

    #high_prob_df, low_prob_df, filtered_df = filter_blast_table(parsed_blast_table)
    #if args.intermediate_files:
    #    high_prob_df.to_csv(
    #        os.path.join(out_dir, 'high_prob_df.csv'), index=False
    #        )
    #    low_prob_df.to_csv(
    #        os.path.join(out_dir, 'low_prob_df.csv'), index=False
    #        )
    #    filtered_df.to_csv(
    #        os.path.join(out_dir, 'filtered_df.csv'), index=False
    #        )
    #augmented_blast_table, high_prob_df, low_prob_df = retrieve_taxonomic_hierarchy(
    #    high_prob_df, low_prob_df, filtered_df
    #    )
    #if args.intermediate_files:
    #    augmented_blast_table.to_csv(
    #        os.path.join(out_dir, 'augmented_blast_table.csv'), index=False
    #        )
    #    high_prob_df.to_csv(
    #        os.path.join(out_dir, 'high_prob_df1.csv'), index=False
    #        )
    #    low_prob_df.to_csv(
    #        os.path.join(out_dir, 'low_prob_df1.csv'), index=False
    #        )

    #high_prob_taxa_assign_df, high_prob_taxa_count_df = find_parsimonious_taxonomy(high_prob_df)
    #high_prob_taxa_assign_df.to_csv(
    #    os.path.join(out_dir, 'high_prob_taxa_assign_df.csv'), index=False
    #    )
    #if args.intermediate_files:
    #    high_prob_taxa_count_df.to_csv(
    #        os.path.join(out_dir, 'high_prob_taxa_count_df.csv'), index=False
    #        )
    #profile_df, low_prob_profile_df = screen_taxonomic_profile(
    #    high_prob_taxa_assign_df, high_prob_taxa_count_df, low_prob_df, high_prob_df
    #    )
    #if args.intermediate_files:
    #    profile_df.to_csv(
    #        os.path.join(out_dir, 'profile_df.csv'), index=False
    #        )
    #low_prob_profile_df.to_csv(
    #    os.path.join(out_dir, 'low_prob_profile_df.csv'), index=False
    #    )

    ## Test the functional uniformity of each set of hits to determine hit accuracy
    ## Draw up to 10 hits from the high-scoring hits of each query group
    ## Evenly sample groups larger than 10, starting with hit 0
    #scan_list_groups = profile_df.groupby(id_type)
    #sampled_df = scan_list_groups.apply(sample_hits)
    #sampled_df.drop('is_in_profile', axis=1, inplace=True)
    #sampled_df.to_csv(
    #    os.path.join(out_dir, 'sampled_df.csv'), index=False
    #    )

    ## Make fasta files for eggnog-mapper
    ## Header: >(scan_list)scan lists(hit)hit number
    ## Seq: full subject seq for each hit
    ## Sort into fasta files based on superkingdom (4 types of files total)
    #generic_emapper_fasta_fp = os.path.join(
    #    out_dir, os.path.splitext(os.path.basename(fasta_input_fp))[0] + '_eggnog_mapper.faa'
    #    )
    #eggnog_fasta_path_list = make_full_hit_seq_fasta(sampled_df, generic_emapper_fasta_fp)
    #with open(
    #    os.path.join(out_dir, 'eggnog_fasta_path_list.pkl'), 'wb'
    #    ) as f:
    #    pkl.dump(eggnog_fasta_path_list, f, 2)

    #return sampled_df, low_prob_profile_df, high_prob_taxa_assign_df, eggnog_fasta_path_list

def analyze_eggnog_output(args, sampled_df, low_prob_profile_df, high_prob_taxa_assign_df, eggnog_fasta_path_list):

    # Load as dataframe
    # Assign predefined column names
    # Concat annotation dfs
    eggnog_out_fp_list = [
        os.path.join(args.iodir, fasta + '.emapper.annotations') for fasta in eggnog_fasta_path_list
        ]

    eggnog_df = parse_eggnog_mapper_output(eggnog_out_fp_list)
    conserv_func_df = find_coherent_hits(eggnog_df, sampled_df)
    reported_df = condense_hits(conserv_func_df, low_prob_profile_df, high_prob_taxa_assign_df)
    reported_df.to_csv(
        os.path.join(args.iodir, 'reported_df.tsv'),
        sep='\t',
        index=False
        )

def parse_args(test_argv = None):

    parser = argparse.ArgumentParser(
        description = 'BLAST+ and taxonomic identification of peptide sequences'
        )
    subparsers = parser.add_subparsers(dest='command')

    parser_make_emapper_input = subparsers.add_parser(
        'make_emapper_input',
        help=('make fasta files for eggNOG-mapper input: '
              'step 1 of remote_eggnog procedure')
        )

    make_emapper_input_seq_source_group = parser_make_emapper_input.add_mutually_exclusive_group()
    make_emapper_input_seq_source_group.add_argument(
        '--postnovo_seqs',
        help='path to postnovo_seqs.faa, the fasta file of query sequences generated by postnovo'
        )
    parser_make_emapper_input.add_argument(
        '--postnovo_seqs_info',
        help='path to postnovo_seqs_info.tsv, produced at the same time as postnovo_seqs.faa'
        )

    parser_make_emapper_input.add_argument(
        '--diamond',
        help=(
            'path to DIAMOND executable: '
            'DIAMOND is used for fast alignment of query sequences longer than 30 AAs'
            )
        )
    parser_make_emapper_input.add_argument(
        '--diamond_db',
        help=(
            'DIAMOND database path as it would be specified in a DIAMOND search, '
            'e.g., /home/samuelmiller/diamond_db/refseq/refseq.dmnd'
            )
        )
    parser_make_emapper_input.add_argument(
        '--blastp',
        help=(
            'path to blastp executable: '
            'BLAST+ blastp-short is used for alignment of query sequences shorter than 30 AAs'
            )
        )
    parser_make_emapper_input.add_argument(
        '--blast_dbs',
        nargs='+',
        help=(
            'BLAST database paths as they would be specified in a BLAST+ search, '
            'e.g., /home/samuelmiller/blast_db/refseq_protein/refseq_protein '
            'If multiple databases are used, '
            'place in order of most general to specific, '
            'e.g., full RefSeq precedes bacterial RefSeq. '
            'Databases should be constructed from fasta files with makeblastdb '
            'after converting all occurrences of Ile to Leu. '
            'The database (not fasta) files should be in their own directory.'
            )
        )
    parser_make_emapper_input.add_argument(
        '--max_seqs_per_blast_instance',
        type=int,
        default=1000,
        help='maximum number of query seqs per BLAST+ instance'
        )
    parser_make_emapper_input.add_argument(
        '--taxonmap',
        help=(
            'Filepath to prot.accession2taxid.gz (from NCBI) '
            'for retrieving taxonomy from accession IDs: '
            'both zipped and unzipped files should be present in the directory '
            )
        )
    parser_make_emapper_input.add_argument(
        '--local_eggnog',
        default=False,
        action='store_true',
        help='flag to create files for local eggNOG'
        )
    parser_make_emapper_input.add_argument(
        '--cores',
        type=int,
        default=1,
        help='number of cores to use'
        )
    parser_make_emapper_input.add_argument(
        '--intermediate_files',
        default=False,
        action='store_true',
        help='flag to write intermediate files for debugging'
        )
    parser_make_emapper_input.add_argument(
        '--redo_without_search',
        default=False,
        action='store_true',
        help='flag to make fasta files after DIAMOND and BLAST output have been generated'
        )

    parser_analyze_emapper_output = subparsers.add_parser(
        'analyze_emapper_output',
        help='analyze eggNOG-mapper + HMMR annotation output'
        )
    parser_analyze_emapper_output.add_argument(
        '--iodir',
        help=('directory containing sampled_df.csv, '
              'low_prob_profile_df.csv, '
              'high_prob_taxa_assign_df.csv, '
              'and eggnog_fasta_path_list.pkl (all produced by make_emapper_input step), '
              'and eggNOG-mapper annotation files')
        )

    if test_argv:
        args = parser.parse_args(test_argv)
    else:
        args = parser.parse_args()
        check_args(parser, args)

    set_global_vars(args)

    return args

def set_global_vars(args):

    global cores

    if args.command == 'make_emapper_input':
        cores = args.cores

def check_args(parser, args):

    if args.command == 'make_emapper_input':

        if args.postnovo_seqs == None:
            parser.error('query fasta input must be provided')
        if args.postnovo_seqs != None:
            if not os.path.exists(args.postnovo_seqs):
                parser.error(args.postnovo_seqs + ' does not exist')

        if args.redo_without_search == False:
            if args.diamond == None:
                parser.error('diamond filepath needed')
            if not os.path.exists(args.diamond):
                parser.error(args.diamond + ' does not exist')
            if args.diamond_db == None:
                parser.error('DIAMOND database filepath must be provided')
            if not os.path.exists(args.diamond_db):
                parser.error(args.diamond_db + ' does not exist')
            if args.taxonmap == None:
                parser.error('taxonmap filepath must be provided')
            if not os.path.exists(args.taxonmap):
                parser.error(args.taxonmap + ' does not exist')

            if args.blastp == None:
                parser.error('blastp filepath needed')
            if not os.path.exists(args.blastp):
                parser.error(args.blastp + ' does not exist')
            if args.blast_dbs == None:
                parser.error('BLAST+ database filepath(s) must be provided')
            for db_fp in args.blast_dbs:
                db_dir = os.path.dirname(db_fp)
                db_name = os.path.basename(db_fp)
                if not os.path.exists(db_dir):
                    parser.error(db_dir + ' does not exist')
                if not os.path.exists(os.path.join(db_dir, db_name + '.pal')):
                    parser.error(db_name + ' does not exist in ' + db_dir)

        if args.max_seqs_per_blast_instance < 1:
            parser.error(str(args.max_seqs_per_blast_instance) + ' must be a positive number')

        if args.cores < 1 or args.cores > cpu_count():
            parser.error(str(cpu_count()) + ' cores are available')

    # analyze_eggnog_output subcommand
    elif args.command == 'analyze_emapper_output':
        if not os.path.exists(args.iodir):
            parser.error(args.iodir + ' does not exist')
        if (
            not os.path.exists(os.path.join(args.iodir, 'sampled_df.csv')) or 
            not os.path.exists(os.path.join(args.iodir, 'low_prob_profile_df.csv')) or
            not os.path.exists(os.path.join(args.iodir, 'high_prob_taxa_assign_df.csv')) or 
            not os.path.exists(os.path.join(args.iodir, 'eggnog_fasta_path_list.pkl'))
            ):
            parser.error(
                'the following files must be present in ' + 
                args.iodir + 
                ': sampled_df.csv, '
                'low_prob_profile_df.csv, '
                'high_prob_taxa_assign_df.csv, '
                'eggnog_fasta_path_list.pkl'
                )

def make_fasta_input(seq_table_fp):
    '''
    Make a fasta file from a csv input formatted as a column of headers and a column of seqs
    '''

    global fasta_input_fp
    fasta_input_fp = os.path.join(
        os.path.dirname(seq_table_fp),
        os.path.splitext(seq_table_fp)[0] + '.faa'
        )
    seq_table = pd.read_csv(seq_table_fp, header=None)
    seq_table.columns = ['headers', 'seqs']
    seq_table['headers'] = seq_table['headers'].apply(
        lambda header: '>' + header
        )
    seq_table['seqs'] = seq_table['seqs'].apply(
        lambda seq: utils.remove_mod_chars(seq=seq)
        )
    fasta_input_series = seq_table.stack()
    fasta_input_series.to_csv(fasta_input_fp, index=False, header=False)
    fasta_list = fasta_input_series.tolist()

    return fasta_list

def split_blast_fasta(fasta_list, max_seqs_per_process):
    '''
    To run multiple BLAST+ processes at once,
    split the fasta file into a number equal to the cores available.
    Returns a list of the new fasta filepaths.
    '''

    split_fasta_fp_list = []
    fasta_basename = os.path.splitext(
        os.path.basename(fasta_input_fp)
        )[0]
    parent_fasta_size = len(fasta_list) / 2
    if parent_fasta_size % int(parent_fasta_size) > 0:
        raise ValueError('The fasta input must have an even number of lines.')
    child_fasta_size = int(parent_fasta_size / cores)
    remainder = parent_fasta_size % cores

    if child_fasta_size + remainder < max_seqs_per_process:
        for core in range(cores):
            child_fasta_list = fasta_list[
                core * child_fasta_size * 2: (core + 1) * child_fasta_size * 2
                ]
            child_fasta_fp = os.path.join(
                out_dir, fasta_basename + '_' + str(core + 1) + '.blastp-short.faa'
                )
            with open(child_fasta_fp, 'w') as child_fasta_file:
                for line in child_fasta_list:
                    child_fasta_file.write(line)
            split_fasta_fp_list.append(child_fasta_fp)
        with open(child_fasta_fp, 'a') as child_fasta_file:
            child_fasta_list = fasta_list[cores * child_fasta_size * 2:]
            for line in child_fasta_list:
                child_fasta_file.write(line)
    else:
        fasta_line = 0
        child_fasta_count = 1
        while fasta_line < len(fasta_list):
            child_fasta_list = fasta_list[
                fasta_line: fasta_line + max_seqs_per_process * 2
                ]
            child_fasta_filename = os.path.join(
                out_dir, fasta_basename + '_' + str(child_fasta_count) + '.blastp-short.faa'
                )
            with open(child_fasta_filename, 'w') as child_fasta_file:
                for line in child_fasta_list:
                    child_fasta_file.write(line)
            split_fasta_fp_list.append(child_fasta_filename)
            fasta_line += max_seqs_per_process * 2
            child_fasta_count += 1

    return split_fasta_fp_list

def run_diamond(fasta_fp, diamond_fp, db_fp, taxonmap, cores):
    
    subprocess.call([
        diamond_fp,
        'blastp',
        '--threads', str(cores),
        '--db', db_fp,
        '--query', fasta_fp,
        '--taxonmap', taxonmap,
        '--more-sensitive',
        '--out', os.path.splitext(fasta_fp)[0] + '.out',
        '--outfmt', '6', 'qseqid', 'sseqid', 'evalue', 'bitscore', 'staxids', 'salltitles',
        '--evalue', '0.1',
        '--max-hsps', '1',
        '--unal', '1',
        '--block-size', '20',
        '--index-chunks', '1'
        ])

def run_blast(db_fp_list, fasta_fp_list, blastp_fp):

    # Record where the blast db's will be loaded into memory
    original_db_fp_dict = OrderedDict()
    loaded_db_fp_dict = OrderedDict()
    for db_fp in db_fp_list:
        db_name = os.path.basename(db_fp)
        original_db_fp_dict[db_name] = db_fp
        loaded_db_fp_dict[db_name] = os.path.join(
            os.path.join('/dev/shm', db_name), db_name
            )

    # Set the number of heavyweight BLAST+ processes (more memory-intensive)
    # and number of threads (less memory-intensive) for each process
    max_processes_allowed = min([cores, 16])
    num_threads = cores // max_processes_allowed

    # Make a list of all the BLAST+ commands to be executed
    commands = []
    out_fp_list = []
    for db_name, loaded_db_fp in loaded_db_fp_dict.items():
        for fasta_fp in fasta_fp_list:
            # Incorporate the name of the db into the BLAST+ output file
            out_fp = os.path.splitext(fasta_fp)[0] + '_' + db_name + '.out'
            commands.append([
                blastp_fp,
                '-task', 'blastp-short',
                '-db', loaded_db_fp,
                '-query', fasta_fp,
                '-out', out_fp,
                '-evalue', '1000000',
                '-max_target_seqs', '500',
                '-max_hsps', '1',
                '-comp_based_stats', '0',
                '-num_threads', str(num_threads),
                '-outfmt', '6 qseqid sacc evalue bitscore salltitles'
                ])
            out_fp_list.append(out_fp)

    # Run the BLAST+ commands, loading and unloading db's from mem as needed
    processes = []
    running_commands = []
    dbs_in_mem = []
    instances_started = 0
    while len(processes) > 0 or instances_started == 0:
        while len(processes) < max_processes_allowed and instances_started < len(fasta_fp_list) * len(db_fp_list):
            command = commands[instances_started]
            running_commands.append(command)
            # Unload any BLAST db's not in use
            dbs_in_use = []
            for running_command in running_commands:
                dbs_in_use.append(running_command[running_command.index('-db') + 1])
            dbs_in_use = list(set(dbs_in_use))
            for db_in_mem in dbs_in_mem:
                if db_in_mem not in dbs_in_use:
                    subprocess.call(['rm', '-r', os.path.dirname(db_in_mem)])
            # Load the BLAST db into memory if not already loaded
            loaded_db_fp = command[command.index('-db') + 1]
            if loaded_db_fp not in dbs_in_mem:
                db_name = os.path.basename(loaded_db_fp)
                subprocess.call([
                    'rm', '-rf',
                    os.path.dirname(loaded_db_fp)
                    ])
                subprocess.call([
                    'mkdir',
                    os.path.dirname(loaded_db_fp)
                    ])
                db_file_list = glob.glob(os.path.join(os.path.dirname(original_db_fp_dict[db_name]), '*'))
                subprocess.call(
                    ['cp'] + 
                    db_file_list + 
                    [os.path.dirname(loaded_db_fp)]
                    )
                dbs_in_mem.append(loaded_db_fp)

            process = subprocess.Popen(command)
            processes.append(process)
            instances_started += 1
        for i, process in enumerate(processes):
            if process.poll() is not None:
                processes.remove(process)
        time.sleep(30)

    # Unload any residual BLAST db's
    for db_in_mem in dbs_in_mem:
        subprocess.call(['rm', '-r', os.path.dirname(db_in_mem)])

    return out_fp_list

#def run_blast(fasta_fp_list, blastp_fp, db_fp):
    
#    processes = []
#    instances_started = 0
#    while len(processes) > 0 or instances_started == 0:
#        while len(processes) < cores and instances_started < len(fasta_fp_list):
#            fasta_fp = fasta_fp_list[instances_started]
#            out_fp = os.path.join(
#                os.path.dirname(fasta_fp),
#                os.path.splitext(os.path.basename(fasta_fp))[0] + '.out'
#                )
#            process = subprocess.Popen([
#                blastp_fp,
#                '-task', 'blastp-short',
#                '-db', db_fp,
#                '-query', fasta_fp,
#                '-out', out_fp,
#                '-evalue', '1000000',
#                '-max_target_seqs', '500',
#                '-max_hsps', '1',
#                '-comp_based_stats', '0',
#                '-outfmt', '6 qseqid sgi sacc evalue bitscore staxids salltitles',
#                ])
#            processes.append(process)
#            instances_started += 1
#            #print('number of instances started: ' + str(instances_started), flush=True)
#        for i, process in enumerate(processes):
#            #print('process poll for ' + str(i) + ': ' + str(process.poll()), flush=True)
#            if process.poll() is not None:
#                processes.remove(process)
#        # REMOVE
#        #pl = subprocess.Popen(['ps'], stdout=subprocess.PIPE).communicate()[0]
#        #print(pl, flush=True)
#        time.sleep(30)

    ## Script name modified
    ##blast_batch_fp = resource_filename('postnovo', 'bashscripts/blast_batch.sh')
    #blast_batch_fp = resource_filename('postnovo', 'bashscripts/blast_batch_simple.sh')
    #with open(blast_batch_fp) as blast_batch_template_file:
    #    blast_batch_template = blast_batch_template_file.read()
    #temp_blast_batch_script = blast_batch_template
    #temp_blast_batch_script = temp_blast_batch_script.replace('FASTA_FILES=', 'FASTA_FILES=({})'.format(' '.join(fasta_fp_list)))
    #temp_blast_batch_script = temp_blast_batch_script.replace('MAX_PROCESSES=', 'MAX_PROCESSES={}'.format(cores - 1))
    #temp_blast_batch_script = temp_blast_batch_script.replace('BLASTP_PATH=', 'BLASTP_PATH={}'.format(blastp_fp))
    #temp_blast_batch_script = temp_blast_batch_script.replace('DB_DIR=', 'DB_DIR={}'.format(db_fp))
    ## Script name modified
    ##temp_blast_batch_fp = os.path.join(os.path.dirname(blast_batch_fp), 'blast_batch~.sh')
    #temp_blast_batch_fp = os.path.join(os.path.dirname(blast_batch_fp), 'blast_batch_simple~.sh')
    #with open(temp_blast_batch_fp, 'w') as temp_blast_batch_file:
    #    temp_blast_batch_file.write(temp_blast_batch_script)
    #os.chmod(temp_blast_batch_fp, 0o777)
    #subprocess.call([temp_blast_batch_fp])

def parse_blast_table(fasta_input_fp, raw_blast_table):
    '''
    Parse the different parts of qseqid into cols
    Retrieve the query seqs from the fasta file
    '''

    # REVISIT
    postnovo_merged_headers = ['scan_list', 'xle_permutation', 'precursor_mass', 'seq_score', 'seq_origin'] + blast_hdr[1:]

    # Split the info in the headers into cols of info
    qseqid_list = raw_blast_table['qseqid'].tolist()
    # qseqid format is, ex., (scan_list)1,2(xle_permutation)0(precursor_mass)1000.000(seq_score)0.55(seq_origin)postnovo
    qseqid_list = [qseqid.split('(scan_list)')[1] for qseqid in qseqid_list]
    temp_list_of_lists = [qseqid.split('(xle_permutation)') for qseqid in qseqid_list]
    scan_col = pd.Series(
        [temp_list[0] for temp_list in temp_list_of_lists])
    temp_list_of_lists = [temp_list[1].split('(precursor_mass)') for temp_list in temp_list_of_lists]
    permut_col = pd.Series(
        [temp_list[0] for temp_list in temp_list_of_lists])
    temp_list_of_lists = [temp_list[1].split('(seq_score)') for temp_list in temp_list_of_lists]
    mass_col = pd.Series(
        [temp_list[0] for temp_list in temp_list_of_lists])
    temp_list_of_lists = [temp_list[1].split('(seq_origin)') for temp_list in temp_list_of_lists]
    score_col = pd.Series(
        [temp_list[0] for temp_list in temp_list_of_lists])
    origin_col = pd.Series(
        [temp_list[1] for temp_list in temp_list_of_lists])
    parsed_blast_table = pd.concat(
        [scan_col,
            permut_col,
            mass_col,
            score_col,
            origin_col,
            raw_blast_table[raw_blast_table.columns[1:]].reset_index(drop=True)],
        axis = 1)
    parsed_blast_table.columns = postnovo_merged_headers

    # Add a hit column to df
    scan_list_list = parsed_blast_table[id_type].tolist()
    last_scan_list = scan_list_list[0]
    hit_list = [0]
    for scan_list in scan_list_list[1:]:
        if scan_list != last_scan_list:
            last_scan_list = scan_list
            hit_list.append(0)
        else:
            hit_list.append(hit_list[-1] + 1)
    parsed_blast_table['hit'] = hit_list

    seq_table = tabulate_fasta(fasta_input_fp)
    merged_blast_table = seq_table.merge(
        parsed_blast_table, on=['scan_list', 'xle_permutation']
        )

    return merged_blast_table

def optimize_blast_table(parsed_blast_table_list):

    # Make a set of all ID's
    id_list = []
    grouped_parsed_blast_table_list = []
    for parsed_blast_table in parsed_blast_table_list:
        id_list += parsed_blast_table[id_type].tolist()
        grouped_parsed_blast_table_list.append(parsed_blast_table.groupby(id_type))
    id_set = set(id_list)
    parsed_blast_table_list = []
    # Loop through each ID
    for id in id_set:
        lowest_evalue = max_float
        lowest_evalue_table_index = 0
        # Find the blast result with the lowest evalue
        for i, grouped_parsed_blast_table in enumerate(grouped_parsed_blast_table_list):
            try:
                evalue = grouped_parsed_blast_table.get_group(id)['evalue'].min()
                if evalue < lowest_evalue:
                    lowest_evalue = evalue
                    lowest_evalue_table_index = i
            except KeyError:
                pass
        # Put the best blast result in a list of df's
        # If no evalue < "non-random" cutoff, use the first results from the db list
        if lowest_evalue < taxa_profile_evalue:
            parsed_blast_table_list.append(
                grouped_parsed_blast_table_list[lowest_evalue_table_index].get_group(id))
        else:
            for grouped_parsed_blast_table in grouped_parsed_blast_table_list:
                try:
                    parsed_blast_table_list.append(
                        grouped_parsed_blast_table.get_group(id))
                    break
                except KeyError:
                    pass
    parsed_blast_table = pd.concat(parsed_blast_table_list, ignore_index=True)

    return parsed_blast_table

def tabulate_fasta(fasta_input_fp):

    raw_fasta_input = pd.read_table(fasta_input_fp, header = None)
    fasta_headers_list = raw_fasta_input.ix[::2, 0].tolist()
    seq_col = raw_fasta_input.ix[1::2, 0]
    seq_col.index = range(len(seq_col))

    # header format is, ex., >(scan_list)1,2(xle_permutation)0(precursor_mass)1000.000(seq_score)0.55(seq_origin)postnovo
    fasta_headers_list = [
        fasta_header.strip('>(scan_list)') for fasta_header in fasta_headers_list
        ]
    temp_list_of_lists = [header.split('(xle_permutation)') for header in fasta_headers_list]
    scan_col = pd.Series(
        [temp_list[0] for temp_list in temp_list_of_lists])
    temp_list_of_lists = [
        temp_list[1].split('(precursor_mass)') for temp_list in temp_list_of_lists
        ]
    permut_col = pd.Series(
        [temp_list[0] for temp_list in temp_list_of_lists])
    seq_table = pd.concat([scan_col, permut_col, seq_col], axis = 1)
    seq_table.columns = ['scan_list', 'xle_permutation', 'seq']

    seq_table['len'] = seq_table['seq'].apply(lambda seq: len(seq))

    return seq_table

def filter_blast_table(blast_table):
    '''
    Filter the blast table by evalue,
    sorting into high- and low-probability sub-tables.
    '''

    blast_table['evalue'] = blast_table['evalue'].apply(float)
    blast_table['bitscore'] = blast_table['bitscore'].apply(float)
    id_set_list = sorted(list(set(blast_table[id_type].tolist())))
    high_prob_df_list = []
    low_prob_df_list = []
    filtered_df_list = []
    gb = blast_table.groupby(id_type)
    for id in id_set_list:
        id_df = gb.get_group(id)
        if id_df['evalue'].min() <= taxa_profile_evalue:
            high_prob_df = id_df[id_df['bitscore'] == id_df['bitscore'].max()]
            high_prob_df_list.append(high_prob_df)
            filtered_df_list.append(high_prob_df)
        else:
            low_prob_df_list.append(id_df)
            filtered_df_list.append(id_df)

    high_prob_df = pd.concat(high_prob_df_list, ignore_index=True)
    low_prob_df = pd.concat(low_prob_df_list, ignore_index=True)
    filtered_df = pd.concat(filtered_df_list, ignore_index=True)

    # If a BLAST database was constructed without taxids, they must be recovered
    if pd.isnull(filtered_df['taxid']).any():
        df_needing_taxids = filtered_df[pd.isnull(filtered_df['taxid'])]
        filtered_df.loc[df_needing_taxids.index, 'taxid'] = retrieve_taxids(df_needing_taxids)
        high_prob_df.drop('taxid', axis=1, inplace=True)
        high_prob_df = high_prob_df.merge(
            filtered_df[[id_type, 'hit', 'taxid']], how='inner', on=[id_type, 'hit']
            )
        low_prob_df.drop('taxid', axis=1, inplace=True)
        low_prob_df = low_prob_df.merge(
            filtered_df[[id_type, 'hit', 'taxid']], how='inner', on=[id_type, 'hit']
            )

    return high_prob_df, low_prob_df, filtered_df

def retrieve_taxids(df):

    # The only way of accurately retrieving the taxid from the information at hand
    # is not by directly querying with the taxonomic name in the subject title,
    # as this can be ambiguous (e.g., 'Bacillus' is a bacterium and walking stick insect),
    # but by querying the protein db with the accession.
    # Assume that no two organisms share a taxonomic name in the sample.
    # Find the list of unique taxonomic names in the sample.
    # Sample the first accession corresponding to this name as a representative query.

    df['taxon'] = df['stitle'].apply(get_taxon)
    unique_taxon_list = df['taxon'].unique().tolist()
    one_percent_number_taxa = len(unique_taxon_list) / 100 / cores
    
    ## Single process
    #accession_list = []
    #print_percent_progress_fn = partial(utils.print_percent_progress_singlethreaded,
    #                                    procedure_str = 'Representative accession retrieval progress: ',
    #                                    one_percent_total_count = one_percent_number_taxa * cores)
    #child_initialize(df, print_percent_progress_fn)
    ##single_var_get_representative_accession = partial(get_representative_accession,
    ##                                                  df = df,
    ##                                                  print_percent_progress_fn = print_percent_progress_fn)
    #for taxon in unique_taxon_list:
    #    accession_list.append(get_representative_accession(taxon))

    # Multiprocess
    print_percent_progress_fn = partial(utils.print_percent_progress_multithreaded,
                                        procedure_str = 'Representative accession retrieval progress: ',
                                        one_percent_total_count = one_percent_number_taxa,
                                        cores = cores)
    child_initialize(df, print_percent_progress_fn)
    #single_var_get_representative_accession = partial(get_representative_accession,
    #                                                  df = df,
    #                                                  print_percent_progress_fn = print_percent_progress_fn)
    multiprocessing_pool = Pool(cores,
                                initializer=child_initialize,
                                initargs=(df,
                                          print_percent_progress_fn)
                                )
    accession_list = multiprocessing_pool.map(get_representative_accession, unique_taxon_list)
    multiprocessing_pool.close()
    multiprocessing_pool.join()

    ## Single process
    #unique_taxid_list = []
    #print_percent_progress_fn = partial(utils.print_percent_progress_singlethreaded,
    #                                    procedure_str = 'Taxid retrieval progress: ',
    #                                    one_percent_total_count = one_percent_number_taxa * cores)
    #single_var_get_taxid = partial(get_taxid,
    #                               print_percent_progress_fn = print_percent_progress_fn)
    #for accession in accession_list:
    #    unique_taxid_list.append(single_var_get_taxid(accession))

    # Multiprocess
    print_percent_progress_fn = partial(utils.print_percent_progress_multithreaded,
                                        procedure_str = 'Taxid retrieval progress: ',
                                        one_percent_total_count = one_percent_number_taxa,
                                        cores = cores)
    single_var_get_taxid = partial(get_taxid,
                                   print_percent_progress_fn = print_percent_progress_fn)
    multiprocessing_pool = Pool(cores)
    unique_taxid_list = multiprocessing_pool.map(single_var_get_taxid, accession_list)
    multiprocessing_pool.close()
    multiprocessing_pool.join()

    taxon_taxid_dict = OrderedDict().fromkeys(unique_taxon_list)
    for i, taxon in enumerate(taxon_taxid_dict):
        taxon_taxid_dict[taxon] = unique_taxid_list[i]

    taxid_list = []
    for taxon in df['taxon'].tolist():
        taxid_list.append(taxon_taxid_dict[taxon])
    #df['taxid'] = taxid_list
    #df.drop('taxon', axis=1, inplace=True)

    ## Pull down the taxid given the taxon string
    #unique_taxon_list = df['taxon'].unique().tolist()
    #unique_taxon_dict = {}.fromkeys(unique_taxon_list)
    #one_percent_number_taxa = len(unique_taxon_list) / 100 / cores

    ### Single process
    ##unique_taxid_list = []
    ##print_percent_progress_fn = partial(utils.print_percent_progress_singlethreaded,
    ##                                    procedure_str = 'Taxid retrieval progress: ',
    ##                                    one_percent_total_count = one_percent_number_taxa * cores)
    ##single_var_get_taxid = partial(get_taxid,
    ##                               print_percent_progress_fn = print_percent_progress_fn)
    ##for taxon in unique_taxon_list:
    ##    unique_taxid_list.append(single_var_get_taxid(taxon))

    ## Multiprocess
    #print_percent_progress_fn = partial(utils.print_percent_progress_multithreaded,
    #                                    procedure_str = 'Taxid retrieval progress: ',
    #                                    one_percent_total_count = one_percent_number_taxa,
    #                                    cores = cores)
    #single_var_get_taxid = partial(get_taxid,
    #                               print_percent_progress_fn = print_percent_progress_fn)
    #multiprocessing_pool = Pool(cores)
    #unique_taxid_list = multiprocessing_pool.map(single_var_get_taxid, unique_taxon_list)
    #multiprocessing_pool.close()
    #multiprocessing_pool.join()

    #for i, taxon in enumerate(unique_taxon_list):
    #    unique_taxon_dict[taxon] = unique_taxid_list[i]

    ## The query procedure sometimes times out indefinitely for certain queries,
    ## so clean up taxids that were not retrieved
    #cleanup_number = 1
    #while 'timeout' in unique_taxon_dict.values():
    #    missed_taxa = [taxon for taxon in unique_taxon_dict if unique_taxon_dict[taxon] == 'timeout']

    #    one_percent_number_taxa = len(missed_taxa) / 100 / cores

    #    ## Single process
    #    #additional_taxid_list = []
    #    #print_percent_progress_fn = partial(utils.print_percent_progress_singlethreaded,
    #    #                                    procedure_str = 'Taxid clean-up retrieval #' + str(cleanup_number) + ' progress: ',
    #    #                                    one_percent_total_count = one_percent_number_taxa * cores)
    #    #single_var_get_taxid = partial(get_taxid,
    #    #                               print_percent_progress_fn = print_percent_progress_fn)
    #    #for taxon in missed_taxa:
    #    #    additional_taxid_list.append(single_var_get_taxid(taxon))

    #    # Multiprocess
    #    print_percent_progress_fn = partial(utils.print_percent_progress_multithreaded,
    #                                        procedure_str = 'Taxid clean-up retrieval #' + str(cleanup_number) + ' progress: ',
    #                                        one_percent_total_count = one_percent_number_taxa,
    #                                        cores = cores)
    #    single_var_get_taxid = partial(get_taxid,
    #                                   print_percent_progress_fn = print_percent_progress_fn)
    #    multiprocessing_pool = Pool(cores)
    #    missed_taxid_list = multiprocessing_pool.map(single_var_get_taxid, missed_taxa)
    #    multiprocessing_pool.close()
    #    multiprocessing_pool.join()

    #    cleanup_number += 1

    #    for i, taxon in enumerate(missed_taxa):
    #        unique_taxon_dict[taxon] = missed_taxid_list[i]

    #taxid_list = []
    #for taxon in df['taxon'].tolist():
    #    taxid_list.append(unique_taxon_dict[taxon])
    #df['taxid'] = taxid_list
    #df.drop('taxon', axis=1, inplace=True)

    return taxid_list

def get_taxon(stitle):

    closed_bracket_count = 0
    open_bracket_count = 0
    for i, char in enumerate(stitle[::-1]):
        if char == ']':
            closed_bracket_count += 1
        elif char == '[':
            open_bracket_count += 1
        if open_bracket_count == closed_bracket_count:
            return stitle[-i: -1]
    print(stitle + 'did not work')

def get_representative_accession(taxon):

    print_percent_progress_fn()

    accession = df[df['taxon'] == taxon].iloc[0]['accession']

    return accession

def child_initialize(_df, _print_percent_progress_fn):

    global df, print_percent_progress_fn

    df = _df
    print_percent_progress_fn = _print_percent_progress_fn

def get_taxid(accession, print_percent_progress_fn):

    print_percent_progress_fn()

    no_response_count = 0
    no_response_count_limit = 5

    while True:
        try:
            esummary = Entrez.read(Entrez.esummary(db='protein', id=accession))
            taxid = esummary[0]['TaxId']
            try:
                int(taxid)
                break
            except ValueError:
                print('Null taxid returned: ', esummary)
        except:
            print('Waiting for accession: ' + accession, flush=True)
            time.sleep(2)

    return taxid

#def get_taxid(taxon, print_percent_progress_fn):

#    print_percent_progress_fn()

#    no_response_count = 0
#    no_response_count_limit = 5

#    while True:
#        try:
#            taxid = Entrez.read(Entrez.esearch(db='Taxonomy', term='\"' + taxon + '\"'))['IdList'][0]
#            break
#        except:
#            if no_response_count == no_response_count_limit:
#                return 'timeout'
#            print('Waiting for taxon ' + str(taxon), flush=True)
#            no_response_count += 1
#            if '\'' in taxon:
#                taxon = taxon.replace('\'', '')
#            time.sleep(2)

#    return taxid

def retrieve_taxonomic_hierarchy(high_prob_df, low_prob_df, filtered_df):
    '''
    Retrieve taxonomic hierarchy from NCBI taxonomy db given taxid
    '''

    unique_taxid_list = filtered_df['taxid'].unique().tolist()
    unique_taxid_dict = OrderedDict().fromkeys(unique_taxid_list)
    one_percent_number_taxid = len(unique_taxid_list) / 100 / cores
    rank_dict = OrderedDict().fromkeys(search_ranks)
    search_ranks_set = set(search_ranks)

    ## Single process
    #lineage_lists = []
    #print_percent_progress_fn = partial(
    #    utils.print_percent_progress_singlethreaded,
    #    procedure_str = 'Taxonomic hierarchy recovery progress: ',
    #    one_percent_total_count = one_percent_number_taxid * cores
    #    )
    #single_var_query_entrez_taxonomy_db = partial(
    #    query_entrez_taxonomy_db,
    #    rank_dict = rank_dict, 
    #    search_ranks_set = search_ranks_set, 
    #    print_percent_progress_fn = print_percent_progress_fn
    #    )
    #for taxid in unique_taxid_list:
    #    lineage_lists.append(single_var_query_entrez_taxonomy_db(taxid))

    # Multiprocess
    print_percent_progress_fn = partial(
        utils.print_percent_progress_multithreaded,
        procedure_str = 'Taxonomic hierarchy recovery progress: ',
        one_percent_total_count = one_percent_number_taxid,
        cores = cores
        )
    single_var_query_entrez_taxonomy_db = partial(
        query_entrez_taxonomy_db,
        rank_dict = rank_dict, 
        search_ranks_set = search_ranks_set, 
        print_percent_progress_fn = print_percent_progress_fn
        )
    multiprocessing_pool = Pool(cores)
    lineage_lists = multiprocessing_pool.map(
        single_var_query_entrez_taxonomy_db, unique_taxid_list
        )
    multiprocessing_pool.close()
    multiprocessing_pool.join()

    for i, taxid in enumerate(unique_taxid_list):
        unique_taxid_dict[taxid] = lineage_lists[i]

    lineage_table_row_list = []
    for taxid in filtered_df['taxid'].tolist():
        lineage_table_row_list.append(unique_taxid_dict[taxid])
    lineage_table = pd.DataFrame(lineage_table_row_list, columns = search_ranks)
    augmented_blast_table = pd.concat([filtered_df, lineage_table], axis=1)

    merge_df = augmented_blast_table[[id_type, 'hit'] + search_ranks]
    high_prob_df = high_prob_df.merge(merge_df, on=[id_type, 'hit'])
    low_prob_df = low_prob_df.merge(merge_df, on=[id_type, 'hit'])

    return augmented_blast_table, high_prob_df, low_prob_df

def query_entrez_taxonomy_db(taxid, rank_dict, search_ranks_set, print_percent_progress_fn):
    '''
    Given a taxid, retrieve the taxon and lineage from NCBI Taxonomy db
    '''

    print_percent_progress_fn()

    # Occasionally, Entrez will return corrupted information
    bad_lineage = True
    q_count = 0
    while bad_lineage and q_count < 5:
        q_count += 1
        while True:
            try:
                taxon_info = Entrez.read(Entrez.efetch(db='Taxonomy', id=taxid))
                break
            except:
                print('Waiting for taxid ' + str(taxid), flush=True)
                time.sleep(2)
        taxon = taxon_info[0]['ScientificName']
        taxon_rank = taxon_info[0]['Rank']
        lineage_info = taxon_info[0]['LineageEx']

        taxon_ranks_set = set()
        if taxon_rank in search_ranks_set:
            rank_dict[taxon_rank] = taxon
            taxon_ranks_set.add(taxon_rank)
        for entry in lineage_info:
            rank = entry['Rank']
            if rank in search_ranks_set:
                rank_dict[rank] = entry['ScientificName']
                taxon_ranks_set.add(rank)
        for rank in search_ranks_set.difference(taxon_ranks_set):
            rank_dict[rank] = ''

        lineage_list = [level for level in rank_dict.values()]
        if lineage_list[-1] not in superkingdoms:
            print('Querying again due to bad lineage: ', taxid, taxon_info)
        else:
            bad_lineage = False

    return lineage_list

def find_parsimonious_taxonomy(df):

    df.set_index(id_type, inplace=True)
    list_of_taxa_assignment_rows = []
    for id in df.index.get_level_values(id_type).unique():
        id_table = df.loc[[id]]
        id_table = id_table.drop_duplicates(subset = ['taxid'])
        for rank_index, rank in enumerate(search_ranks):
            most_common_taxon_count = Counter(id_table[rank]).most_common(1)[0]
            if most_common_taxon_count[0] != '' and pd.notnull(most_common_taxon_count[0]):
                if most_common_taxon_count[1] >= taxon_assignment_threshold * len(id_table):
                    id_table.reset_index(inplace = True)
                    try:
                        representative_row = id_table.ix[
                            id_table[
                                id_table[rank] == most_common_taxon_count[0]
                                ][rank].first_valid_index()
                            ]
                    except:
                        print('id: ' + str(id), flush=True)
                        print('rank: ' + str(rank), flush=True)
                        print('is pd.null:')
                        print(pd.isnull(most_common_taxon_count[0]))
                        sys.exit()
                    list_of_taxa_assignment_rows.append(
                        [id] + \
                            [representative_row['seq']] + \
                            [representative_row['precursor_mass']] + \
                            [representative_row['seq_score']] + \
                            [representative_row['seq_origin']] + \
                            rank_index * ['N/A'] + \
                            representative_row[rank:].tolist()
                        )
                    break
        else:
            representative_row = id_table.iloc[0]
            list_of_taxa_assignment_rows.append(
                [id] + \
                    [representative_row['seq']] + \
                    [representative_row['precursor_mass']] + \
                    [representative_row['seq_score']] + \
                    [representative_row['seq_origin']] + \
                    len(search_ranks) * ['N/A']
                )

    taxa_assignment_table = pd.DataFrame(
        list_of_taxa_assignment_rows,
        columns=['scan_list', 'seq', 'precursor_mass', 'seq_score', 'seq_origin'] + search_ranks)

    taxa_count_table = pd.DataFrame()
    for rank in search_ranks:
        taxa_counts = Counter(taxa_assignment_table[rank])
        taxa_count_table = pd.concat(
            [taxa_count_table,
             pd.Series([taxon for taxon in taxa_counts.keys()], name = rank + ' taxa'),
             pd.Series([count for count in taxa_counts.values()], name = rank + ' counts')],
             axis = 1
             )

    df.reset_index(inplace=True)

    return taxa_assignment_table, taxa_count_table

def screen_taxonomic_profile(high_prob_taxa_assign_df, high_prob_taxa_count_df, low_prob_df, high_prob_df):
    '''
    Screen the low-prob results for those in the high-prob taxonomic profile
    '''

    gold_taxa_profile_dict = OrderedDict()
    silver_taxa_profile_dict = OrderedDict()
    for level in ['family', 'genus', 'species']:
        name_list_raw = high_prob_taxa_count_df[level + ' taxa'].tolist()
        count_list_raw = high_prob_taxa_count_df[level + ' counts'].tolist()
        name_list = []
        count_list = []
        for i in range(len(name_list_raw)):
            if pd.notnull(name_list_raw[i]):
                name_list.append(name_list_raw[i])
                count_list.append(count_list_raw[i])
        count_dict = {name_list[i]: count_list[i] for i in range(len(name_list))}
        # Taxa in profile must not be singletons
        gold_taxa_profile_dict[level] = [i for i in count_dict if count_dict[i] > 5]
        silver_taxa_profile_dict[level] = [i for i in count_dict if 5 >= count_dict[i] > 1]

    is_in_profile_list = [0 for i in range(len(low_prob_df))]
    for level in ['family', 'genus', 'species']:
        gold_taxa_profile_list = gold_taxa_profile_dict[level]
        silver_taxa_profile_list = silver_taxa_profile_dict[level]
        low_prob_taxa_list = low_prob_df[level].tolist()
        for i, low_prob_taxon in enumerate(low_prob_taxa_list):
            if not is_in_profile_list[i]:
                if low_prob_taxon in gold_taxa_profile_list:
                    is_in_profile_list[i] = 1
                elif low_prob_taxon in silver_taxa_profile_list:
                    is_in_profile_list[i] = 2
    low_prob_df['is_in_profile'] = is_in_profile_list

    low_prob_profile_df = low_prob_df[low_prob_df['is_in_profile'] > 0]
    profile_df = pd.concat([high_prob_df, low_prob_profile_df], ignore_index=True)

    return profile_df, low_prob_profile_df

def sample_hits(parent_scan_group, sample_size = 10):

    # Consider hits within 2 bits of the top hit in the profile group
    scan_group = parent_scan_group[
        parent_scan_group['bitscore'] >= (parent_scan_group['bitscore'].max() - 2)
        ]
    # If the taxonomic profile lacks confidence, consider the full table
    if 1 not in scan_group['is_in_profile'].values:
        scan_group = parent_scan_group

    group_size = len(scan_group)
    if group_size <= 10:
        return scan_group
    else:
        group_rows = list(range(group_size))
        div, mod = divmod(len(group_rows), sample_size)
        sample_rows = [group_rows[i * div + min(i, mod)] for i in range(sample_size)]
        return scan_group.iloc[sample_rows]

def make_full_hit_seq_fasta(sampled_df, generic_emapper_fasta_fp):
    
    accession_list = sampled_df['accession'].tolist()
    one_percent_number_subject_seqs = len(accession_list) / 100 / cores

    # Multiprocess
    multiprocessing_pool = Pool(cores)
    print_percent_progress_fn = partial(
        utils.print_percent_progress_multithreaded,
        procedure_str = 'Full subject sequence recovery progress: ',
        one_percent_total_count = one_percent_number_subject_seqs,
        cores = cores
        )
    single_var_query_ncbi_protein = partial(
        query_ncbi_protein,
        print_percent_progress_fn = print_percent_progress_fn
        )
    full_hit_seq_list = multiprocessing_pool.map(single_var_query_ncbi_protein, accession_list)
    multiprocessing_pool.close()
    multiprocessing_pool.join()

    sampled_df[id_type] = sampled_df[id_type].apply(str)
    sampled_df['hit'] = sampled_df['hit'].apply(str)
    sampled_df['full seq'] = full_hit_seq_list

    eggnog_fasta_path_list = []
    for superkingdom in superkingdoms:
        superkingdom_df = sampled_df[sampled_df['superkingdom'] == superkingdom]
        # Only write to file if there is information for the superkingdom
        if len(superkingdom_df) > 0:
            superkingdom_id_list = superkingdom_df[id_type].tolist()
            superkingdom_hit_list = superkingdom_df['hit'].tolist()
            superkingdom_seq_list = superkingdom_df['full seq'].tolist()
            superkingdom_header_list = [
                '>' + '(' + id_type + ')' + superkingdom_id_list[i] + '(hit)' + superkingdom_hit_list[i]
                for i in range(len(superkingdom_id_list))
                ]
            superkingdom_header_seq_dict = {
                superkingdom_header_list[i]: superkingdom_seq_list[i]
                for i in range(len(superkingdom_header_list))
                }

            # online emapper with hmmr requires <=5000 seqs/file
            if local_eggnog:
                hmmr_seq_count_limit = 1000000000
            else:
                hmmr_seq_count_limit = 5000
            dirname = os.path.dirname(generic_emapper_fasta_fp)
            basename = os.path.splitext(os.path.basename(generic_emapper_fasta_fp))[0]

            previous_last_row = 0
            for i in range(len(superkingdom_header_list) // hmmr_seq_count_limit + 1):
                superkingdom_write_path = os.path.join(
                    dirname, basename + '.' + superkingdom.lower() + '_' + str(i) + '.faa'
                    )
                eggnog_fasta_path_list.append(basename + '.' + superkingdom.lower() + '_' + str(i) + '.faa')
                with open(superkingdom_write_path, 'w') as f:
                    for j, header in enumerate(superkingdom_header_list[previous_last_row:]):
                        f.write(header + '\n')
                        f.write(superkingdom_header_seq_dict[header] + '\n')
                        if j+1 == hmmr_seq_count_limit:
                            previous_last_row = (j+1) * (i+1)
                            break

    return eggnog_fasta_path_list
    
def query_ncbi_protein(accession, print_percent_progress_fn):

    print_percent_progress_fn()

    while True:
        try:
            full_seq = Entrez.read(
                Entrez.efetch(db='Protein', id=accession, retmode='xml')
                )[0]['GBSeq_sequence']
            break
        except:
            print('Waiting for ' + accession, flush=True)
            time.sleep(2)

    return full_seq

def parse_eggnog_mapper_output(eggnog_out_fp_list):

    eggnog_df = pd.DataFrame(columns=eggnog_output_headers)
    for out_fp in eggnog_out_fp_list:
        if os.path.exists(out_fp):
            if os.stat(out_fp).st_size > 0:
                eggnog_output_df = pd.read_csv(out_fp, sep='\t', header=None, names=eggnog_output_headers, comment='#')
                eggnog_df = pd.concat([eggnog_df, eggnog_output_df], ignore_index=True)

    # Split header into two cols for scan lists and hits
    query_list = eggnog_df['query'].tolist()
    query_list = [query.split('(' + id_type + ')')[1] for query in query_list]
    temp_list_of_lists = [query.split('(hit)') for query in query_list]
    id_list = [temp_list[0] for temp_list in temp_list_of_lists]
    hit_list = [temp_list[1] for temp_list in temp_list_of_lists]
    eggnog_df.drop('query', axis=1, inplace=True)
    eggnog_df[id_type] = id_list
    eggnog_df['hit'] = hit_list

    return eggnog_df

def find_coherent_hits(eggnog_df, sampled_df):

    # Need to account for queries that do not have an eggnog annotation
    # Need a table of all id + hit
    # Groupby id, make sorted list of hit
    # Sorted list of hits for id must match that from eggnog annotations to call annotations conserved

    conserv_func_df_list = []
    eggnog_df_groups = eggnog_df.groupby(id_type)
    sampled_df_groups = sampled_df.groupby(id_type)
    for id in list(set(eggnog_df[id_type])):
        eggnog_df_group = eggnog_df_groups.get_group(id)
        sampled_df_group = sampled_df_groups.get_group(id)
        # If all annotated seqs have the same description string
        if (eggnog_df_group['eggnog hmm desc'] == eggnog_df_group['eggnog hmm desc'].iloc[0]).all():
            # If all the queries (sampled blast hits) are present in the annotation file
            if len(eggnog_df_group) == len(sampled_df_group):
                conserv_func_df_list.append(eggnog_df_group)
    conserv_func_df = pd.concat(conserv_func_df_list, ignore_index=True)
    return conserv_func_df

def condense_hits(conserv_func_df, low_prob_profile_df, high_prob_taxa_assign_df):

    # Condense rows of each group
    conserv_func_df = conserv_func_df.fillna('')
    id_set_list = list(set(conserv_func_df['scan_list'].tolist()))
    parsed_conserv_func_df_cols_dict = OrderedDict(
        [(header, []) for header in parsed_conserv_func_df_headers_list])
    parsed_conserv_func_df = pd.DataFrame()
    parsed_conserv_func_df[id_type] = id_set_list
    conserv_func_df_groups = conserv_func_df.groupby(id_type)
    # Loop through each group
    for id in id_set_list:
        conserv_func_df_group = conserv_func_df_groups.get_group(id)
        # Go through each col in group
        # seed ortholog: set intersection
        parsed_conserv_func_df_cols_dict['seed ortholog'].append(
            ','.join(list(set(conserv_func_df_group['seed ortholog']))))
        # evalue: ignore
        # score: ignore
        # predicted name: set intersection
        parsed_conserv_func_df_cols_dict['predicted name'].append(
            ','.join(list(set(conserv_func_df_group['predicted name']))))
        # go terms: set intersection
        list_of_lists = conserv_func_df_group['go terms'].apply(lambda x: x.split(',')).tolist()
        l = [i for sublist in list_of_lists for i in sublist]
        parsed_conserv_func_df_cols_dict['go terms'].append(','.join(list(set(l))))
        # kegg pathways: set intersection
        list_of_lists = conserv_func_df_group['kegg pathways'].apply(lambda x: x.split(',')).tolist()
        l = [i for sublist in list_of_lists for i in sublist]
        parsed_conserv_func_df_cols_dict['kegg pathways'].append(
            ','.join(list(set(l))))
        # tax scope: set intersection
        parsed_conserv_func_df_cols_dict['tax scope'].append(
            ','.join(list(set(conserv_func_df_group['tax scope']))))
        # eggnog ogs: set intersection
        list_of_lists = conserv_func_df_group['eggnog ogs'].apply(lambda x: x.split(',')).tolist()
        l = [i for sublist in list_of_lists for i in sublist]
        parsed_conserv_func_df_cols_dict['eggnog ogs'].append(
            ','.join(list(set(l))))
        # best og: set intersection of first substring before |
        parsed_conserv_func_df_cols_dict['best og'].append(
            ','.join(list(set(
                conserv_func_df_group['best og'].apply(lambda x: x.split('|')[0])))))
        # cog cat: set intersection
        parsed_conserv_func_df_cols_dict['cog cat'].append(
            ','.join(list(set(conserv_func_df_group['cog cat']))))
        # eggnog hmm desc: first row (all rows the same)
        parsed_conserv_func_df_cols_dict['eggnog hmm desc'].append(
            conserv_func_df_group['eggnog hmm desc'].iloc[0])
        # scan_list: already placed in parsed_conserv_func_df
        # hit: ignore
    for header, col in parsed_conserv_func_df_cols_dict.items():
        parsed_conserv_func_df[header] = col

    low_prob_profile_df.set_index(id_type, inplace=True)
    id_set_list = set(id_set_list).intersection(set(low_prob_profile_df.index.tolist()))
    low_prob_profile_df = low_prob_profile_df.loc[id_set_list]
    low_prob_profile_df.reset_index(inplace=True)
    low_prob_profile_df = low_prob_profile_df.drop('is_in_profile', axis=1)
    low_prob_profile_taxa_assign_df, low_prob_profile_taxa_count_df = \
        find_parsimonious_taxonomy(low_prob_profile_df)

    if 'hit' in high_prob_taxa_assign_df.columns:
        high_prob_taxa_assign_df.drop('hit', axis=1, inplace=True)

    profile_taxa_assign_df = pd.concat([high_prob_taxa_assign_df, low_prob_profile_taxa_assign_df], ignore_index=True)

    reported_df = parsed_conserv_func_df.merge(profile_taxa_assign_df, how='inner', on=id_type)
    reported_df.fillna('', inplace=True)
    reported_df.replace('N/A', '', inplace=True)
    # Put double quotes around scan_list values
    return reported_df

if __name__ == '__main__':
    main()